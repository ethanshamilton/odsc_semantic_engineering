[
    {
        "title": "Knowledge Cast – Rachad Najjar of GE Vernova",
        "url": "https://enterprise-knowledge.com/knowledge-cast-rachad-najjar-of-ge-vernova/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "ai",
            "knowledge management"
        ],
        "article_type": "podcast",
        "content": "Enterprise Knowledge CEO Zach Wahl speaks with Rachad Najjar, Organizational Learning Leader at GE Vernova. GE Vernova brings together GE’s portfolio of energy businesses including Power, Wind, Electrification and Digital businesses.\n\nIn their conversation, Zach and Rachad cover creating adaptive spaces for knowledge exchange, overcoming barriers to knowledge sharing, and the importance of psychological safety. They explore strategies for making tacit knowledge accessible, including community engagement and leveraging technology, while also delving into the challenges of measuring the impact of knowledge management initiatives.\n\n﻿ ﻿ ﻿ ﻿\n\n\n\n\n\n\n\n\n\nIf you would like to be a guest on Knowledge Cast, contact Enterprise Knowledge for more information."
    },
    {
        "title": "What Every CEO Needs to Know About Semantic Layers",
        "url": "https://enterprise-knowledge.com/what-every-ceo-needs-to-know-about-semantic-layers/",
        "categories": [
            "ai",
            "knowledge graphs data modeling"
        ],
        "tags": [
            "ai",
            "enterprise ai",
            "semantic layer"
        ],
        "article_type": "blog",
        "content": "Recently, we at Enterprise Knowledge have been talking a lot about Semantic Layers, defining what they are , and even what they aren’t . We’ve detailed the various technical components and the business value of each, presented logical diagrams, and identified many core use cases for which they may be applied. We continue to generate a depth of detail on the topic, and when I consider many of our clients, CEOs and top executives of some of the world’s largest companies, I want to make sure we’re adequately answering the question of why a senior executive should care about a Semantic Layer.\n\nTo answer that question succinctly, a Semantic Layer can be an organization’s pathway to achieving Artificial Intelligence . A properly implemented Semantic Layer delivers the quality inputs, the relationships and vocabularies, and the necessary connections and infrastructure to make AI real for most organizations. If AI initiatives have failed or are stalled, implementing a Semantic Layer will deliver all the necessary elements to make AI successful at your organization.\n\nTo further answer the question, I explored the top concerns of global CEOs from The Conference Board , effectively, what CEOs are losing sleep over. These are listed below, with specific examples of how a Semantic Layer can help to address each of these challenges.\n\n\n\n\n\nEconomic Downturn/Recession\n\nA Semantic Layer can deliver a comprehensive, aligned, and integrated view of an organization’s operations. Regardless of the industry, this means an executive will have the ability to understand bottlenecks, spot inefficiencies, and even predict areas of savings. This translates to better decision-making and meaningful competitive advantage. One of our customers in the publishing and education industries had previously lacked the ability to look all the way across their diverse organization’s operations. They didn’t even have a consistent definition of core business concepts like what a product is, and therefore had no ability to understand profit and loss on a product by product basis. The next two quarters following the implementation of their Search and Intelligent Chatbot (powered by the Semantic Layer), they achieved a marked increase in margin as the company refocused sales efforts around their highest profitability products. More importantly, they attained a framework that allows them to evaluate, in real-time, which products are the most profitable.\n\nInflation\n\nAs I mentioned above, a complete view of an organization’s operations will enable executives to spot hidden or unrealized costs and proactively make decisions to remove or shift those costs to keep operating costs lower. A Semantic Layer can also be used to counteract the impacts of inflation. Most customized customer services powered by a Semantic Layer can be used to retain customers even when they’re more cost conscious due to inflation. Similarly, with properly targeted and customized marketing, down to the individual level, organizations can more successfully win new customers, regardless of market conditions.\n\nGlobal Political Instability\n\nA Semantic Layer cannot prevent Global Political Instability (at least, not yet). However, it can help an organization predict where global politics may impact business, either from a market or supply perspective, helping executives to shift operations proactively. We did something like this for a global development bank. For them, we developed a knowledge graph as a component to their semantic layer (they dubbed it “the brain”) that identified trends between all of their global development projects, the countries and regions in which they were run, and recommended lessons learned to prevent repeats of the same mistakes or errors on future initiatives that might encounter the same issues. A Semantic Layer can use machine learning to help identify trends a human might not see, flagging the trends for executives to make these proactive decisions before they impact business.\n\nHigher Borrowing Costs\n\nThough it can’t control bank interest rates, a Semantic Layer can serve as a cost reduction tool in many ways, ranging from improved productivity, to identification of inefficiencies, even automatically identifying unnecessary redundancies in various stages of the supply chain. For one of our clients, we applied a Semantic Layer to map their “cradle to grave” operations, helping them to cut out unnecessary steps that were slowing down their operations. It not only saved them money, but reduced their production cycle and time to market by nearly ten percent.\n\nLabor Shortages\n\nWhen applied against employee onboarding, learning, development, and performance, a Semantic Layer can be a critical tool in developing and retaining employees, helping to alleviate potential labor shortages. A Semantic Layer can serve as a map of employees, skills, and tenure that allows executives to identify current and future labor shortages and focus training and recruiting efforts on the most important skills.  For one services company, we incorporated the organization’s products, services, customers, and sales pipeline into the Semantic Layer, providing the organization’s executives with predictions of where they may have future labor shortages based on their employee demographics and sales pipeline, allowing them to proactively fill gaps through training or hiring before they even existed.\n\nRapidly Advancing AI Technology\n\nA Semantic Layer can be an organization’s pathway to achieving Artificial Intelligence. If a CEO is losing sleep over how their organization can harness the power of AI, before getting left behind, a Semantic Layer is their answer. Given that the biggest hurdle for enterprise AI adoption is the lack of a comprehensive understanding of organization’s data and the black box nature of how most AI solutions are trained, a Semantic Layer plays a crucial role in providing a bridge between the complex underlying data and the end-users or AI applications. Specifically, by aggregating heterogeneous data and aligning business terminologies across various departments, we were able to employ components of Semantic Layer for a large financial institution to train complex AI algorithms based on their risk and controls library and management processes resulting in an approximately 80% improved accuracy of their risk identification process and alignment of relevant mitigation strategies.\n\nHigher Labor Costs\n\nTo expand on the point I made regarding labor shortages, a Semantic Layer can be a key tool in employee retention. Greater employee retention means lower costs related to recruiting and (re)training new employees, thereby helping to decrease overall labor costs. Another significant cost regarding labor is transfer of skills and knowledge from an organization’s most tenured and experienced resources to those who are new to an organization. An AI chatbot based on a Semantic Layer can help organizations get reliable answers to less experienced employees so that they can provide value more quickly. For a federal research institute with a retiring workforce, we leveraged the Semantic Layer to automate the identification of tenured experts (from project data leveraging taxonomies/metadata) and capture high value knowledge from their most experienced employees, then further used additional components of the the Semantic Layer and AI to automatically deliver that knowledge to new employees at their point of need. This not only improved employee development, it resulted in greater organizational efficiency and safety.\n\nRegulation\n\nA Semantic Layer can help an organization to stay up-to-date on all regulations and laws, ensuring they are compliant at the national, regional, and local levels. Moreover, especially regarding data privacy, security, and information delivery, a Semantic Layer can be a key tool to ensure an organization follows each regulation and is fully compliant. One large organization that deals with a myriad of individually identifiable health information (IIHI), came to EK after incurring a massive fine for the improper management of this information. We developed a Semantic Layer for them, which not only helped them achieve compliance, but also helped them spot ancillary systems with risky information management practices that needed to be secured.\n\nGlobal Financial Crisis\n\nEach of the points I’ve already made have demonstrated how an organization can be more prepared for a global financial crisis, by having a complete understanding of their supply chain, a greater mastery of costs and redundancies, and a predictive view of their market and employee needs. An organization that has harnessed a Semantic Layer won’t be impervious to a financial crisis, but they’ll be more prepared for it, more able to adjust to it, and quicker to respond when emergencies arise.\n\nShifting Consumer/Customer Buying Behaviors\n\nA Semantic Layer isn’t just about understanding your organization, it’s also about connecting data about customer behavior and understanding your customers and clients and delivering fully customized experiences and communications for them. Buying behaviors will always shift, but the organizations leveraging a Semantic Layer connecting knowledge and data assets across sales, marketing, and customer service will have greater intelligence and awareness around these shifts, meaning they’ll be ready for the shifts and more likely to retain or even expand their customer base while their competitors are left wondering what’s happening. For a large retail customer, we helped them build a graph of their customers and their buying habits, then matched individually assembled communications and offerings to them using the Semantic Layer, resulting in improved engagement and sales, including new engagement from previously dormant customers.\n\nConclusion\n\nFrom the view of the CEO, a Semantic Layer, put simply, is about enterprise intelligence. Rather than piecing together answers from disparate systems, wasting time commissioning costly and time intensive research efforts, and taking a “best guess” approach, organization’s armed with a Semantic Layer will have the ability to get the right answers when they need them (or before), respond in advance to rapidly changing factors, and operate not just with Artificial Intelligence on their side, but with Organizational Intelligence.\n\nIf you’re ready to sleep better at night by harnessing the power of a Semantic Layer, let us know . This is not wishful thinking of the future – this is what we can, and have, achieved today."
    },
    {
        "title": "Dynamic Content",
        "url": "https://enterprise-knowledge.com/dynamic-content/",
        "categories": [
            "advanced content"
        ],
        "tags": [
            "advanced content",
            "dynamic content"
        ],
        "article_type": "infographic",
        "content": "Every day, people are inundated with information at a rapid and ever-increasing pace. This creates pressure to make sure that your audience is getting the content they need when and how they need it. Creating high-quality content quickly and efficiently requires thinking of your content not as a web page or PDF but as a set of flexible, reusable building blocks that can be assembled into the format that meets your employees’ or customers’ needs. These blocks are sometimes referred to as intelligent content, modular content, atomic content, or component content. A dynamic content model leverages those reusable components to build personalized experiences. Read on for a crash course on Dynamic Content .\n\nInterested in turning your content into a strategic business asset? Contact us today !"
    },
    {
        "title": "Breaking Down Enterprise AI, Part I: Insight Track",
        "url": "https://enterprise-knowledge.com/breaking-down-enterprise-ai-part-i-insight-track/",
        "categories": [
            "ai"
        ],
        "tags": [
            "descriptive ai",
            "diagnostic ai",
            "insights",
            "predictive ai",
            "prescriptive ai"
        ],
        "article_type": "blog",
        "content": "2023 was an incredibly exciting year for artificial intelligence. Several technology trends – distributed computing, transformer models, and large repositories of data – converged to produce powerful generative AI products like ChatGPT and Midjourney. These products have helped generate excitement for the field of AI overall, but the vast spectrum of available AI approaches may leave you unsure of what type of AI to pursue for your own enterprise goals.\n\nEnterprise Knowledge’s Enterprise AI (e-AI) division has experience delivering enterprise solutions throughout the field of AI, which we break down into the following categories:\n\n\n\n\n\n\n\nIn this blog we will focus on the insight track, which includes Descriptive, Diagnostic, Predictive, and Prescriptive AI. These categories of AI are focused on analyzing and modeling data to produce insights. A sequel to this blog will focus on the action track, including Expert, Generative, and Agentic AI, which focuses on systems that can utilize data to perform actions.\n\nEnterprise AI begins primarily with data. Making good decisions based on past data is a core function of human intelligence, and AI techniques from the insight track are designed to augment this human capacity or build it into automated systems.\n\nDescriptive AI is the most basic level of the insight track. Describing and analyzing patterns in data is the most fundamental activity AI can assist with, and forms the foundation of more advanced forms of AI. Diagnostic AI assists with the identification of normal and abnormal patterns in the data, indicating where time and resources should be directed. Predictive AI extrapolates trends from past data into the future, assisting with sound decision-making about the future. Finally, Prescriptive AI makes use of techniques from the previous types of AI to recommend the best actions based on your enterprise goals.\n\nLet’s take a quick tour through each category of the insight track to understand what drives them and how they can be applied.\n\n\n\nDescriptive AI\n\nDescriptive intelligence is the capability of assessing the shapes of data by its statistical features like size, distribution, or variance and presenting the assessments in relatable ways. Considered through this lens, Descriptive AI is surprisingly widespread. Automating these analyses and applying them to vast pools of data is where these simple techniques augment human capability. For example, visualizing datasets of enterprise transactions and customer relationships can be performed by an AI system for interpretation by human agents.\n\nBringing these representations closer to the point of need for enterprise employees can enhance the immediate decision-making process, ideally resulting in faster and more frequent success. Building systems that allow access to descriptive AI at the point of need is one of the simplest ways an organization can begin to utilize artificial intelligence.\n\nSee this case study as an example of our experience implementing descriptive AI in systems that can help organizations make the most of their data.\n\n\n\nDiagnostic AI\n\nDiagnostic intelligence is the capability of distinguishing between normal and abnormal patterns in data. While descriptive AI is best applied to pools of data, diagnostic AI makes the most sense when applied to streams of data that must be monitored. Automatically distinguishing between normal and abnormal patterns in the data can help identify how well enterprise systems are functioning. This will help people allocate their attention more effectively in what could be a vast sea of enterprise data. Diagnostic AI systems have been used to look for patterns that are characteristic of malicious behavior like fraud or network intrusions, or to look for patterns that indicate a malfunction like the poor vital signs of a sick patient or the overconsumption of energy in an unoccupied building.\n\nDiagnostic AI can help identify issues in a system faster and more frequently than human monitoring, as these systems can operate consistently around the clock and detect patterns that might escape the attention of a human. This makes Diagnostic AI well-suited for augmenting defensive or conservative functions like risk mitigation and resource allocation.\n\nEK has led the design of Digital Twin knowledge graphs , which are a great example of a diagnostic AI form factor.\n\n\n\nPredictive AI\n\nPredictive intelligence involves estimating future outcomes based on past data. From anticipating other drivers on the road to choosing the next word in a sentence, predictive intelligence is extremely useful for determining what actions to take in the future. While a perfect oracle has not been built (and probably never will), imperfect predictions are essential for the process of planning and are even a feature of certain industries like entertainment.\n\nCreating a predictive AI system involves creating some sort of model of past data and using it to generate predictions based on some variable or set of variables. This can range from logical models where the rules and logical relationships represent the outcomes of past analyses to dynamic neural network models that can create a wide range of predictive models with the same underlying architecture.\n\nApplying predictive AI in the enterprise involves creating these models from past datasets and applying them in augmentative tools for people or as integrated modules of an action-oriented AI system like generative or agentic AI. This can help people leverage the information stored in past data in their decision-making process, or bring predictive intelligence to automated workers like self-driving cars or virtual customer service agents.\n\nWe have often developed predictive AI systems in collaboration with clients. For example, we worked on a solution that would help our client predict their energy consumption for sustainability practices.\n\n\n\nPrescriptive AI\n\nPrescriptive intelligence takes us to the event horizon of action without actually crossing. Prescriptive AI involves creating a system that can integrate the factual information from previous levels of the insight track and output a suggestion on what should be done. Not only do we have some idea of abnormal data or a prediction of the future to base our action on, but we also have the judgment necessary to make a decision based on that information.\n\nSome of the simplest examples of this sort of system exist in the recommendation algorithms that power our social media and entertainment industries. These sorts of prescriptions look like the combination of multiple data interpretations: “We noticed you have watched several sci-fi movies in the past (descriptive AI), and we have also noticed that many sci-fi movie enthusiasts  have enjoyed this particular film (diagnostic AI). We think you would like it! (predictive AI) You should watch it! (prescriptive AI)”. Prescriptive AI isn’t just the suggestion that you should do something, it’s the process of combining these prior conclusions into the suggestion that’s sent to the user.\n\nPrescriptive AI can be extremely useful for growing engagement with an AI system because if it is accurate it is likely to significantly decrease the amount of friction users experience with their tools. On one hand, this allows the knowledge and information stored in your system to reach users more frequently; on the other hand, over-optimizing for the engagement of your users can be risky in consumer products. Complex trade-offs like these abound in the field of AI, which highlights the importance of defining a strategy for its use.\n\nEK has extensive experience developing prescriptive AI systems based on semantic recommendation knowledge graphs. For an example, see this case study . In addition to recommendation systems, knowledge graphs and semantic technology can be used to help improve the explainability of AI systems, provide contextual information to LLMs, and synthesize data from across the organization into a unified semantic layer for improved findability and data management, which will benefit your AI efforts even further.\n\n\n\nConclusion\n\nWe hope this article has been helpful for understanding how you can leverage artificial intelligence. If you have an idea for your organization and need help building it, or if you are looking for help strategizing your organizational approach to the entire spectrum of AI please contact us ! We’ll be happy to help."
    },
    {
        "title": "What Isn’t a Semantic Layer?",
        "url": "https://enterprise-knowledge.com/what-isnt-a-semantic-layer/",
        "categories": [
            "knowledge graphs data modeling"
        ],
        "tags": [
            "knowledge management",
            "semantic layer",
            "semantic llm accelerator"
        ],
        "article_type": "infographic",
        "content": "Semantic layers allow organizations to embed context with their organizational data in a way that is systematically interoperable and intuitive to business users. With developments in data and AI, there is a compelling need (and opportunity) for semantic layer development, but that comes along with noise around what semantic layers are and what they offer. My colleague Lulit defined the semantic layer framework, its components, and a sample architecture in her blog “What is a Semantic Layer?”\n\nTo expand on the definition of a Semantic Layer, and moreover to eliminate the noise and confusion around them, I present to you what it is not . Debunking these misconceptions steers organizations from common detours or roadblocks when building semantic layers, such as:\n\nWhile you should be wary of the misconceptions about Semantic Layers, their value as tangible solutions that are driving enterprise AI, reporting, search, and personalization initiatives couldn’t be more real. Starting small with a Semantic LLM Accelerator or a 2-day Semantic Architecture Workshop can help to show value quickly while building a foundation for future scale. If you’ve got foundational components and are looking to see how other organizations have achieved the Semantic Layer, check out our case studies or contact us with questions."
    },
    {
        "title": "What is a Large Language Model (LLM)?",
        "url": "https://enterprise-knowledge.com/what-is-a-large-language-model-llm/",
        "categories": [
            "ai",
            "knowledge graphs data modeling"
        ],
        "tags": [
            "ai",
            "artifical intelligence",
            "chatgpt",
            "large language model",
            "llm",
            "nlp",
            "use cases"
        ],
        "article_type": "blog",
        "content": "In late November of 2022, artificial intelligence (AI) research and development company OpenAI released ChatGPT, an AI chatbot powered by a Large Language Model (LLM). In the following year, the world witnessed a meteoric rise in the usage of ChatGPT and other LLMs across a diverse array of industries and applications. However, what large language models actually are and what they are capable of is often misunderstood. In this blog, I will define LLMs, explore how they work, explain their strengths and weaknesses, and elaborate on a few of the most common LLM use cases for the enterprise.\n\n\n\n\n\nSo, what is a Large Language Model?\n\nIn short, a Large Language Model is an advanced AI model designed to perform Natural Language Processing (NLP) tasks, including interpreting, translating, predicting, and generating coherent, contextually relevant text. LLMs require extensive training on vast textual datasets that contain trillions of words, like Wikipedia and GitHub, which teaches the model to recognize patterns in text. An LLM such as OpenAI’s GPT-4 isn’t doing any “reasoning” like a human does, at least not yet – it is merely generating output that fits the patterns it has learned through training. It can simply be thought of as doing very sophisticated predictions of which words in which context go in what order.\n\n\n\nHow does a Large Language Model work?\n\nAll LLMs operate by leveraging immense, layered networks of interconnected nodes that process and transmit information. The structure of the networks draws inspiration from the interconnectedness of the human brain’s network of neurons. Within this framework, LLMs use so-called transformer models – consisting of an encoder and a decoder – to turn input into output.\n\nIn the process of handling a sequence of input text, a tokenizer algorithm first converts the text into a machine-readable format by breaking down the text into small, discrete units called “tokens” for analysis; tokens themselves are often single words or single letters.\n\nFor example, the sentence “Hello, world!” can be tokenized into [“Hello”,  “,”,  “world”,  “!”].\n\n\n\n\n\nThese tokens are then converted into numerical values known as embedding vectors, which is the format expected by the transformer model. However, because transformers can’t inherently understand the order of words, each embedding vector is combined with a positional encoding. This step ensures the order of the words is taken into account by the model.\n\nAfter the input text is tokenized, it is passed through the encoder to create attention vectors, which are numerical values that help the model determine the relevance and relationship of each token to the others in the input. This helps the LLM capture dependencies and relationships between tokens, giving it the ability to process the context of each token in the sequence.\n\nThe attention vectors are then passed to the decoder to receive an output embedding, which are then converted back into tokens. The decoder process continues until a “STOP” token is output by the transformer, indicating that no more output text should be generated. This process ensures that the generated output considers the relevant information from the input, maintaining coherence and context in the generated text. This is similar to how a human might receive a question, automatically identify the most important aspects of the question, and give an appropriate response that addresses those aspects.\n\n\n\n\n\n\n\nStrengths\n\nLarge language models exhibit several strengths that businesses can capitalize on:\n\n\n\nWeaknesses\n\nIn spite of their strengths, LLMs have numerous weaknesses:\n\n\n\nLLM Use Cases for the Enterprise\n\nLarge language models have many applications that utilize their strengths. However, their weaknesses manifest across all use cases, so businesses must make considerations to prevent complications and mitigate risks. These are some of the most common use cases where we have employed LLMs:\n\nContent generation:\n\nInformation Retrieval:\n\nText Analysis:\n\n\n\nConclusion\n\nIn the past year, large language models have seen an explosion in adoption and innovation, and they aren’t going anywhere any time soon – ChatGPT alone reached 100 million active users in January 2023, and continues to see nearly 1.5 billion website visits per month. The enormous popularity of LLMs is supported by their obvious utility in interpreting, generating, and summarizing text, as well as their applications in a variety of technical and non-technical fields. However, LLMs come with downsides that cannot be brushed aside by any business seeking to use or create one. Due to their non-deterministic and emergent capabilities, businesses should prioritize working with experts in order to properly mitigate risks and capitalize on the strengths of a large language model.\n\nWant to jumpstart your organization’s use of LLMs? Check out our Semantic LLM Accelerator and contact us at [email protected] for more information!"
    },
    {
        "title": "How to Prepare Content for AI",
        "url": "https://enterprise-knowledge.com/how-to-prepare-content-for-ai/",
        "categories": [
            "advanced content",
            "ai",
            "knowledge graphs data modeling",
            "software development"
        ],
        "tags": [
            "ai",
            "content",
            "llm",
            "rag"
        ],
        "article_type": "blog",
        "content": "Artificial Intelligence (AI) enables organizations to leverage and manage their content in exciting new ways, from chatbots and content summarization to auto-tagging and personalization. Most organizations have a copious amount of content and are looking to use AI to improve their operations and efficiency while enabling end users to find relevant information quickly and intuitively.\n\nWith the rise of ChatGPT and other generative AI tools in the last year, there’s a common misconception that you can “do” AI on any content with no preparation. If you want accurate and useful results and insights, however, it requires some upfront work. Understanding how AI interacts with your content and how your content strategy supports AI readiness will set you up for an effective AI implementation.\n\nHow AI Interacts with Content\n\nWhile AI can help in many phases of the content lifecycle, from planning and authoring to discovery, AI usually interacts with existing content in two key ways:\n\nWhen AI examines existing content, it is trying to understand what it is about and how it relates to other concepts within the knowledge domain, and there are steps we can take to help. While this blog is mostly considering how large language models (LLMs) and retrieval augmented generation (RAG) AI interact with content, the steps listed below will prepare content for a variety of other types of AI for both insight and action.\n\nDeveloping a Content Strategy\n\nThe best way to prepare content for AI is to develop a content strategy that addresses the relationships, the structure, the clean up, and the componentization of the content. One key preliminary activity will be to audit your content with the specific lens of AI-readiness, and to assess your organization’s content against the steps listed below.\n\nModel the Knowledge Domain\n\nIn most situations, AI creates internal models to group and cluster information to help the AI respond efficiently to new inputs. AI does a decent job of inferring the relationships between information, but organizations can significantly assist this process by defining an ontology . Ontologies enable organizations to define and relate organizational information, codifying how people, tools, content, topics, and other concepts are related. These models improve findability , support advanced search use cases , and form semantic layers that facilitate the integration of data from multiple sources into consumable formats and user-intuitive structures.\n\nOnce created, an ontology can be used with content to:\n\nModeling an organization’s knowledge domain with an ontology improves AI’s ability to utilize content more effectively and produce more accurate results.\n\nCleanup and Deduplicate the Content\n\nToday’s organizations have too much content and duplicated information. Content is often split between multiple systems due to limitations with legacy tools, user permissions, or the need to support new features and displays. While auditing all of an organization’s content may seem daunting, there are steps an organization can take to streamline the process. Organizations should focus on their NERDy content, identifying the new, essential, reliable, and dynamic content users need to perform their jobs. As part of this focus, organizations reduce content ROT (Redundant, Outdated, Trivial) , improving user trust and experience with organizational information. As part of the cleaning effort, an organization may want to create a centralized authoring platform to maintain content in one place rather than siloed locations. This allows content to be managed in one place, reducing the effort to update content and enabling content reuse . Reusing content helps deduplicate content, removing the need to replicate and update content in multiple places. A content audit, analysis, and clean-up will organize content in an intuitive way for AI and reduce bias from repeated or incorrect information.\n\n\n\nAdd Structure and Standardization\n\nOnce your organization’s knowledge domain is defined, the next step is to create the content models and content types that support that ontology, this is often referred to as content engineering . Content types are the reusable templates that standardize the structure for a particular format of content such as articles, infographics, webinars, and product information, as well as the standard metadata that should be included with that content type (created date, author, department, related subjects, etc.).\n\n\n\nIf we think of Content Types as the cake pan in this analogy, a content model is the Cake Recipe. While the Content Type defines the structure of the content, the Content Model defines the meaning of that content. In the cake analogy, you may have a chocolate cake, a vanilla cake, and a carrot cake; theoretically, any of those recipes could be used in any of the pans. If the content type dictates how , the content model dictates what . In an organization this could look like a content model of a product that includes parts like the product title, the product value proposition, the product features, etc. This content model of a product could then be fit into many content types, such as a brochure, a web page, and an infographic. By creating content models and content types we give the AI model better insight into how the content is connected and the purpose it serves.\n\nThe structure of these templates provides AI with content in a consumable and semantically meaningful format where content sections and metadata are given to the AI model. A crucial part of content engineering  is the creation of a taxonomy to describe the content. Taxonomies should be user-centric, highlighting users’ terminology to talk about content. The terms within a taxonomy and the associated synonyms improve an AI’s capability to utilize the content. Additionally, content types and content models facilitate the consistent display of information and configuration of advanced search features, improving the user experience when looking for and viewing content.\n\nComponentize the Content\n\nOnce the content is structured and cleaned, a common next step is to break up the content into smaller sections according to the content model. This process has many names, such as content deconstruction , content chunking, or the creation of content components. In content deconstruction, structured content is split into smaller semantically meaningful sections. Each section or component has a standalone purpose, even without the context of the original document. Content components are often managed in a component content management system (CCMS) , providing the following benefits:\n\nSimilar to the user benefits, content components provide AI with user-generated components of content as opposed to requiring the AI to perform statistical chunking. The content chunks allow an AI to identify relevant text inputs quickly and more accurately than if fed entire large documents.\n\nConclusion\n\nThrough effective content strategy, content audit, and content engineering, organizations can efficiently manage information and ensure that AI has correct, comprehensive content with semantically meaningful relationships. A well-defined content strategy provides a framework to curate old and new information, allowing organizations to continuously feed information into AI, keeping its internal models up-to-date. A well-structured content audit will ensure preparation time is spent on the areas that will make the most difference in AI-readiness such as structure, standardization, componentization, and relationships across content. Well-thought-out content engineering will enable content reuse and personalization at scale through machine readable structure.\n\nAre you seeking help defining a content strategy, auditing your content for AI-readiness, or training your AI to understand your domain? Contact us and let us know how we can help!\n\nSpecial thank you to James Midkiff for his contributions to the first draft of this blog post!"
    },
    {
        "title": "Extending Taxonomies to Ontologies",
        "url": "https://enterprise-knowledge.com/extending-taxonomies-to-ontologies/",
        "categories": [
            "taxonomy ontology design"
        ],
        "tags": [
            "ontology",
            "ontology design",
            "taxonomy",
            "taxonomy design"
        ],
        "article_type": "white paper",
        "content": "Sometimes the words “taxonomy” and “ontology” are used interchangeably, and while they are closely related, they are not the same thing. They are both considered kinds of knowledge organization systems to support information and knowledge management. Yet there is often a lack of agreement on their definitions, although published standards help define them both. Rather than debating definitions, what is of greater importance is what a taxonomy or ontology enables you to do.\n\nBenefits of Taxonomies and Ontologies\n\nTaxonomies (hierarchical or faceted structured controlled vocabularies of concepts) primarily enhance search and retrieval of content, but they have related benefits. Taxonomy uses and benefits include:\n\nOntologies (semantic models comprising the types/classes, semantic relationships, and attributes of entities) were originally for describing a domain while also supporting inference for learning more about the domain. However, when entities from a taxonomy are combined with an ontology, benefits and capabilities include:\n\n“Content” refers to files, documents, images, intranet pages, spreadsheets, etc. “Data” refers to such things as the information within database records and the cells within tables or spreadsheets. Sometimes people are looking for content, sometimes they are looking for data, and sometimes they are looking for both. Taxonomies focus on connecting users to content, and ontologies focus on data, so a combination of taxonomies and ontologies can connect users to both content and data, in addition to connecting the content and data together.\n\nTaxonomies and Ontologies Combined\n\nTaxonomies and ontologies have different origins (library/information science vs. computer/data science), and thus usually different experts, but these two knowledge organization systems have converged greatly in the past decade. There are two primary reasons for this convergence:\n\nAs mentioned above, there are different definitions for ontologies, and a leading difference concerns whether individual entities are included within the scope of “ontology.” An ontology is either:\n\nThe following pair of diagrams listing different controlled vocabulary and knowledge organization systems illustrate the views of these two different definitions of ontologies.\n\nDepending on how you define ontology, above, a taxonomy can then either\n\nOntologies alone may have taxonomic features of deep hierarchies of classes and subclasses, but without a taxonomy or thesaurus built on the SKOS (Simple Knowledge Organization System) data model, the full range of functionality of alternative labels, labels in other languages, multiple definitions and types of notes, etc. are not supported. Taxonomies provide a linguistic aspect that ontologies alone lack.\n\nOntologies alone would support modeling, exploring, and visualizing entities and their relationships, which may be based on their properties. Ontologies may also support inference reasoning. However, functions involving semantic search, which brings together synonyms and disambiguating homonyms, etc. require taxonomies, thesauri, or other controlled vocabularies.\n\nCreating an Ontology Based on Taxonomies\n\nRegardless of which of the two definitions of ontology you prefer, if you already have a taxonomy, which is often the case, you can extend it to become or or add an ontology and then reap the additional benefits of the combined knowledge organization system. If you have multiple taxonomies and other controlled vocabularies, an ontology can link them together.\n\nWhether you are building a taxonomy, ontology, knowledge graph, or a broader digital transformation for knowledge management, there should be a combination of top down and bottom up approaches to the process. The top-down methods focus on obtaining input from stakeholders, whereas the bottom-up methods focus on analysis of content and data.\n\nThe basic approach to building an ontology , especially a business or enterprise ontology, is to identify groups of things (or “business objects”), which become classes in an ontology, identify relationships between pairs of classes, and identify important characteristics (or attributes) of members of a class. The top-down approach to this task involves interviewing stakeholders and conducting brainstorming sessions and focus group sessions to identify these classes, relationships, and attributes. The bottom-up approach to ontology creation often involves looking at spreadsheets and tables of critical data pertaining to different business objects.\n\nA quicker bottom-up approach to creating ontologies is to look at the taxonomies and controlled vocabularies you already have. Each taxonomy hierarchy, controlled vocabulary, term set, facet, or what is designated as a “concept scheme” in the SKOS model can be considered to be a class in an ontology. Additional classes or subclasses might get added, and some term lists might not be needed in an ontology, but often concept schemes can serve as the basis of classes, one-to-one.\n\nFacets in a faceted taxonomy enable browsing or limiting searches for content items by certain aspects. However, content needs to be limited to that of a similar kind that shares the same facets, such as all product pages, all reports, all employee profiles, or all media files. If we can convert the facets to ontology classes, create new semantic relationships between them, and tag all content, a search application is no longer limited to a certain kind of content or asset. Rather, conditional queries in the same application/user interface can be targeted at any kind of content.\n\nExample: Converting Facets to Classes to Build an Ontology\n\nConsider an example for an organization’s internal knowledge base. There may exist multiple repositories of content and data, each with its own faceted taxonomy and its own user interface.\n\nWe could create classes to reflect the aggregation of all of these facets.\n\nThen we could consider the relationships or links between the classes, and create verb-based semantic relationships. Any class that is a target/object of a relationship can be a target of a search query. The following are just some examples, but not a complete list with all reciprocal relationships.\n\nEmployee knows Subject Employee created File Type Employee possesses Skill Employee basedIn Location Employee belongsTo Division\n\nFile Type hasTopic Subject File Type createdBy Employee File Type belongsTo Division\n\nSubject knownBy Employee Subject topicOf Division Subject topicOf File Type Subject topicOf Event\n\nEvent basedIn Location Event belongsTo Division Event hasTopic Subject\n\nFinally, you should consider what additional data is of importance for the entities in each class, such as contact information for Employees and dates of publication for files and for the occurrence of Events. These would normally not exist in a taxonomy, but should be added to the ontology to support the exploration of more kinds of data.\n\nConclusions\n\nCombining a taxonomy with an ontology provides many benefits and capabilities which a taxonomy alone or an ontology alone (as merely a semantic model) cannot provide.\n\nBuilding an ontology based on one or more existing taxonomies is an efficient and very suitable method of bottom-up development. The existing taxonomies and controlled vocabularies provide a basis for knowledge modeling. Furthermore, by leveraging an existing taxonomy that has already been tagged to content, certain benefits of the ontology will already be in place.\n\nManaging the taxonomy plus the ontology as a semantic layer also has benefits. A taxonomy plus ontology is more flexible and adaptable than a single large ontology, since the taxonomy changes more frequently than does the ontology. Also, more taxonomies and controlled vocabularies can easily be added in the future. There are also several software options for combined taxonomy-ontology creation and management. These applications are based on RDF, including SKOS for taxonomies and RDF-S and OWL for ontologies. This facilitates the technical aspects of extending a taxonomy to become an ontology.\n\nAlthough extending taxonomies to become ontologies is easier than creating ontologies from scratch, it still requires ontology design expertise. For assistance in extending your taxonomies into an ontology, contact us to get started."
    },
    {
        "title": "Graph Machine Learning Recommender POC for Public Safety Agency",
        "url": "https://enterprise-knowledge.com/graph-machine-learning-recommender-poc-for-public-safety-agency/",
        "categories": [
            "knowledge graphs data modeling"
        ],
        "tags": [
            "knowledge graph",
            "knowledge graph accelerator",
            "machine learning",
            "natural language processing",
            "recommendation engine"
        ],
        "article_type": "case study",
        "content": "The Challenge\n\nA government agency responsible for regulating and enforcing occupational safety sought to build a content recommender proof-of-concept (POC) that leverages semantic technologies to model the relevant workplace safety domains. The agency aimed to optimize project planning and construction site design by centralizing information from siloed and unstructured sources and extracting a comprehensive view of potential safety risks and related regulations. Automatically connecting and surfacing this information in a single location via the recommender would serve to minimize time currently spent searching for content and limit burdensome manual efforts, ultimately improving risk awareness and facilitating data-driven decision-making for risk mitigation and regulatory adherence.\n\nThe Solution\n\nThe agency partnered with EK to develop a knowledge graph-powered semantic recommendation engine with a custom front-end. Based on the use case we refined for construction site project planners, we redesigned the agency’s applicable taxonomies and developed an ontology that defined relationships to model the recommendation journey from the user’s inputs of construction site elements to the expected outputs of risks and regulations. With data loaded into the graph from taxonomy values and structured historical data, EK leveraged machine learning (ML) and natural language processing (NLP) techniques to extract data from the agency’s large volume of structured data and generate risk recommendations from user input combinations. EK iterated upon these processes to enrich the data and fine tune the risk prediction models to achieve even more accurate results. Then, based on low-fidelity wireframes collaboratively developed and validated by the client, EK’s software engineers created an interactive front-end for users to view the results and provide feedback, and ultimately deployed the application on cloud infrastructure.\n\nLastly, in addition to the design and development of the initial POC, EK collaborated closely with the client to assess future uses for the application, as well as methods for improving performance and utility. Potential paths for improving the application include developing user feedback mechanisms, expanding the dimensions of analysis for work sites, and expanding the scope of the application to support additional use cases. EK provided the agency with clear recommendations for next steps and paths forward to build upon the POC and further optimize construction site design and planning.\n\nThe EK Difference\n\nEK employed its extensive experience in taxonomy design, ontology design, and data science with specific expertise in the development of recommender systems to capture and model the semantic content of the construction safety domain. Throughout the engagement, EK prioritized close collaboration with the client’s core project team and involved their subject matter experts and stakeholders in taxonomy, ontology, and wireframe design sessions, iteratively soliciting their feedback and domain knowledge to ensure the final product would properly reflect the language and subject matter for the agency’s use case. EK also provided transparency into the development of the recommender, providing thorough technical walkthroughs of the solution. This ensured the agency had all the knowledge required to make informed decisions regarding next steps to scale the solution following the end of our engagement.\n\nThe Results\n\nThe graph-powered recommender solution delivered at the end of the engagement was a compelling POC for the client to consider for long-term application and scale. The recommendation engine provided coherent recommendations in a centralized location to reduce manual efforts for end users and displayed related regulations and supporting metrics to facilitate context-based, data-driven decision-making for construction site planners at the agency. The tailored roadmap to refine and expand the solution offered clear guidance for further data and system improvements to increase the overall utility of the recommender. With this POC and the accompanying roadmap, the agency has a tangible and effective solution with a path to scale to achieve widespread buy-in from across the organization and address more complex use cases in order to maximize the value of the recommender.\n\nThis project was an example of EK’s Knowledge Graph Accelerator offering, delivering the POC to the client in 4 months.\n\nReady to Get Started?"
    },
    {
        "title": "The Top 5 Reasons for a Semantic Layer",
        "url": "https://enterprise-knowledge.com/the-top-5-reasons-for-a-semantic-layer/",
        "categories": [
            "knowledge graphs data modeling"
        ],
        "tags": [
            "ai strategy",
            "data strategy",
            "semantic data layer"
        ],
        "article_type": "blog",
        "content": "Implementing Semantic Layers has become a critical strategic plan for many of our most advanced data clients. A Semantic Layer connects all organizational knowledge assets, including content items (files, videos, media, etc.) via a well defined and standardized semantic framework. If you are unfamiliar with Semantic Layers, read Lulit Tesfaye’s blog What is a Semantic Layer . It provides a great explanation of the Semantic Layer and how it can be implemented. There are a lot of great reasons organizations should implement Semantic Layers. My top five reasons are below.\n\nImproved Findability and Confidence in Data\n\nData continues to grow at an alarming rate. Leaders want their organizations to be data-driven, but their direct reports need to know the data they require and have confidence in that data. A Semantic Layer helps with both of these issues. It uses a graph database and the metadata from your data catalog to offer a best-in-class search that returns data in the context of the business need. For example, if you are looking for all the data sets containing information about the average purchase price of a product, a graph-based search would have a result explaining what the purchase price is and then show all of the data sets that have purchase transactions with price information in them. Many of our retail clients have multiple data feeds from different purchasing systems. Showing all of this information together helps ensure that one of the feeds is not missed.\n\nThe information returned in this type of graph-based custom search is not limited to data sets. We have one client who uses the graph to capture the relationship between a dashboard, the dashboard objects, and the data tables that populate each component. Their graph-based search not only returns data sets, but also the dashboards and dashboard objects that display results. Their IT people use this to develop new dashboards with the correct data sets and their data scientists prioritize the data sets that power the dashboards they already use\n\nGoogle has been using graph search for years. Now, this same technology is available in our data environments.\n\nEnabling AI for Data\n\nAI and ChatGPT are all over the news these days. It is a budget priority for every company executive I speak with. One of the most exciting use cases for Generative AI is the databot. Organizations that implement databots give their business users easy access to the metrics they need to do their job. Rather than trying to build dashboards that anticipate users’ needs, databots allow business users to ask questions of any level of complexity and get answers without knowing or understanding anything about the data behind the result. Software companies in the Semantic Layer are already showing demos of how business users can ask their data complicated natural language questions and get answers back.\n\nDatabots require integration with a Generative AI tool (LLM). This integration will not work without a Semantic Layer. The Semantic Layer, specifically the metadata, taxonomy, and graph framework, provides the context so that LLM tools can properly answer these data-specific questions with organizational context. The importance of the Semantic Layer has been proven in multiple studies. In one study, Juan Sequeda, Dean Allmegang, and Bryan Jacob of data.world produced a benchmark showing how knowledge graphs affect the accuracy of question answering against SQL databases. You can see the results of this study here . Their benchmark evaluated how LLMs answered both high complexity and low complexity questions on both high and low schema data sets. The results are below.\n\nAs these stats show, organizations implementing a Semantic Layer are better equipped to integrate with an LLM. One of the most striking results is that the schema is much less important than the availability of a knowledge graph in question response accuracy. If your organization is looking to integrate the use of LLMs into your data environment, a Semantic Layer is critical.\n\nReporting Across Data Domains\n\nThe Semantic Layer uses a combination of the semantic framework (metadata/ taxonomies/ontologies/knowledge graphs) to map data and related data tools to the entities that business users care about. This approach creates a flexible and more reliable way to manage data across different domains. It gives business users greater access to the information they need in a format that makes sense.\n\nReporting on metrics that cross data domains or systems continues to be challenging for large enterprises. Historically, these organizations have addressed this through complex ETL processes and rigid dashboards that attempt to align and aggregate the information for business users. This approach has several problems, including:\n\nImplementing a Semantic Layer addresses each of these issues. Taxonomies provide a consistent way to categorize data across domains. The taxonomies are implemented as metadata in the data catalogs so business users and data owners can quickly find and align information across their current sources. The Knowledge Graph portion of the Semantic Layer maps data sets and data elements to business objects. These maps can be used to pull information back dynamically without the need for ETL processes. When an ETL process is required for performance purposes, how the data is related is defined in the graph and not in the head of your data developers. ETL routines can be developed against the knowledge graph rather than in code. As the data changes, the map can be updated so that the processes that use that data reflect the new changes immediately.\n\nWe developed a Semantic Layer for a retail client. Once it was in place, they could report on sales transactions from 6 different point-of-sale systems (each with a different format) in a way that used to be done using time-consuming and complicated ETL processes. They were also able to expand their reporting to show links between third-party sales, store sales, and supply chain issues in a single dashboard. This was impossible before the Semantic Layer was in place because they were overly reliant upon a small set of developers and dashboards that only addressed one domain at a time. Instead of constantly building and maintaining complex ETL routines that move data around, our client maps and defines the relationships in the graph and updates the graph or their metadata when changes occur. Business users are seeing more information than they ever have, and they have greater trust in what they are seeing.\n\nImproved Data Governance\n\nData governance is critical to providing business users with data that they have confidence in for proper decision-making. The velocity and variety of today’s data environments makes controlling and managing that data seem almost impossible. Tools from the Semantic Layer are built to address the problem of scale and complexity organizations face. Data catalogs use metadata and built-in workflows to allow organizations to manage similar data sets in similar ways. They also provide data lineage information so that users know how data is used and what has been done to the data files over time. Metadata driven data catalogs give organizations a way to align similar data sets and a framework so that they can be managed collectively rather than individually.\n\nIn addition to data catalogs, ontologies and knowledge graphs can aid in enterprise data governance. Ontologies identify data elements representing the same thing from a business standpoint, even if they are from different source locations or have different field names. Tying similar data elements together in a machine-readable way allows the system to enforce a consistent set of rules automatically. For example, at a large financial institution we worked with, a knowledge graph linked all fields that represented the open date for an account. The customer was a bank with investment accounts, bank accounts, and credit card accounts. Because ontologies linked these fields as account open dates, we could implement constraints that ensured these fields are always filled out, use a standard date format, and have a date in a reasonable timeframe. The ability to automate constraints across many related fields, allows data administrators to scale their processes even as the data they are collecting continues to grow.\n\nStronger Security\n\nThe incremental growth of data has made controlling access to data sets (A.K.A. entitlements) more challenging than ever. Sensitive data, like HR data, must have limited access for those that need to know only. Licensed data could have contractual limitations as to the number of users and may not exist in your organization’s data lake. Often, data is combined from multiple sources. What are the security rules for those new data combinations? The number of permutations and rules as to who can see what across an organization’s data landscape is daunting.\n\nThe Semantic Layer improves the way data entitlements are managed using metadata. The metadata can define the source of the data (for licensed data) as well as the type of data so that sensitive data can be more easily found and flagged. Data administrators can use a data catalog to find licensed data and ensure proper access rules are in place. They can also find data about a sensitive topic, like salaries, and ensure that the proper security measures are in place. Data Lineage, a common feature in catalogs, can also help identify when a newly combined data set needs to be secured and who should see it. Catalogs have gone a long way to solve these security problems, but they are insufficient to solve the growing security challenges.\n\nKnowledge graphs augment the information about data stored in data catalogs to provide greater insight and inference of data entitlements. Graphs map relationships across data and those relationships can be used to identify related data sets that need similar security rules. Because the graph’s relationships are machine-readable, implementation of many of these security rules can be automated. Graphs can also identify how and where data sets are used to identify potential security mismatches. For example, a graph can identify situations where data sets have different security requirements than the dashboards that display them. These situations can be automatically flagged and exposed to data administrators who can proactively align the security between the data and the dashboard.\n\nIn Conclusion\n\nLayers are a natural evolution of the recognition that metadata is a first class citizen in the battle to get the right data to the right people at the right time. The combination of formal metadata and graphs gives data administrators and data users new ways to find, manage, and work with data."
    },
    {
        "title": "Measuring the Value of your Semantic Layer: KPIs for Taxonomies, Ontologies, and Knowledge Graphs",
        "url": "https://enterprise-knowledge.com/measuring-the-value-of-your-semantic-layer-kpis-for-taxonomies-ontologies-and-knowledge-graphs/",
        "categories": [
            "knowledge graphs data modeling",
            "strategy design"
        ],
        "tags": [
            "business value",
            "content",
            "knowledge graph",
            "kpis",
            "ontology",
            "scale",
            "semantic layer",
            "semantic solutions",
            "semantics",
            "taxonomy"
        ],
        "article_type": "blog",
        "content": "Utilizing semantic applications in your business, such as an enterprise taxonomy, ontology, or knowledge graph, can increase efficiency, reduce cognitive load, and improve cohesion across the enterprise, among other benefits. While these benefits are extremely valuable they can be difficult to quantify and measure. Utilizing Key Performance Indicators (KPIs) is a common tactic that can directly tie the semantic integration into the business value it creates. This can be utilized to increase buy-in from leadership, build a common understanding of goals and trajectory, keep projects on track, and to tell powerful stories about the benefits created through these critical investments.\n\n\n\nWhy KPIs?\n\nKPIs are evaluation metrics that are specific, measurable, and goal-oriented. Good KPIs are easy to communicate and translate directly into business value. They are used to support trends over time and can be combined with other measures to tell a story.  They are focused on the  activity at hand, and thus the specific metrics are tied to the use-case, business drivers, and the overall goals of any given initiative.\n\nThe broad objectives frequently associated with semantic tools, such as increased efficiency or more cohesive understanding across the organization can be particularly difficult to quantify and track.  KPIs serve as a bridge, creating qualitative metrics to measure against and serving as course correctors to keep projects focused and on track.  Without KPIs, lofty objectives can hang like rain clouds, hovering with ambiguity over otherwise productive efforts. At EK we have seen the impact of murky objectives or a lack of KPIs manifest through time errantly devoted to the wrong efforts, misunderstandings of the intended outcomes, or even an impediment to generating buy-in from leadership. KPIs quantify ROI and provide clear metrics to measure impact before, during, and after a semantic integration. These ways to see progress throughout a project lifecycle serve to unify and create momentum for businesses.\n\nCreating KPIs that assess the impact of a semantic integration is both a challenge and a skill.  Since KPIs are inherently tied to business objectives and specific use-cases it is hard to provide a standard set of KPIs for the semantic layer. The matrix below provides some example objectives and potential KPIs in 3 key areas: Scale, Content, and Time. More details on each focus area is elaborated on following the graphic.\n\n\n\n\n\nScale\n\nMeasurements of scale can be used to understand the sheer size of the semantic layer and to compare multiple iterations to demonstrate growth, such as the number of entities in a knowledge graph, or terms in a taxonomy. Conversely, the reduction in size of the content or asset library can also be an important measure, for instance when performing deduplication. Reducing the scale of asset libraries can also have a marked impact on additional business motivations such as a reduction in the cost and/or carbon footprint required to support oversized data servers. Additionally, a smaller content library results in a lower cognitive load for end users and less redundant, or incorrect information. In one project , an investment and insurance company was struggling with inefficient search and users were frustrated because the content that was returned to them was frequently out of date or inaccessible. Content auditing, a standardized taxonomy, and auto-tagging workflows were employed and EK identified that about 45% of the content in their existing library was obsolete or outdated.  By cleaning up this content prior to migrating to a new system the scale of the library was reduced. This reduction in size paired with a newly standardized enterprise taxonomy resulted in more accurate content descriptions and a more relevant library for users. This occurrence is far from uncommon and emphasizes the critical nature of content cleanup and maintaining a “right-size” content library.\n\n\n\nContent Management and Fit\n\nCommon measures of accuracy for taxonomies include an understanding of precision ( is the returned information correct? ) as well as recall ( is all the expected information included? ). Assessing content fit is notoriously difficult, generally reliant on Subject Matter Experts and qualitative data, and thus is very tricky to quantify. Focused KPIs can help to quantify this data, for example, if the percentage of correctly auto tagged terms is 75% at the first evaluation mark and 90% at the next point of measure, then the system is 15% more accurate. Measuring accuracy is a critical step in the iterative development of taxonomy and other semantic solutions.  Without an understanding of what is working, it is impossible to understand what changes need to be made. Being able to find and access accurate and up-to-date information is a common pain point that we see among businesses who come to EK. After reducing the size of the content library through content clean up, the business in our earlier example was able to apply a newly formed enterprise taxonomy to their content with an auto-tagging workflow. Analysis of this effort found that the one time auto-tagging process had a 86-99% success rate depending on the content type. This effort greatly increased the accuracy of the metadata describing the content resulting in a better semantic search experience and increased findability across the enterprise.\n\n\n\nTime\n\nOne of the major changes that a well tuned semantic layer can bring about is a measurable amount of time saved. From auto-tagging and other automations to reducing duplication and incorrect information. Time savings increase user satisfaction as well as create a tangible benefit for the company in reduction of “human-hours” spent on any given task. This reduction in hours is an easily measurable reference point, since it can generally translate directly into the labor cost of any given employee. If a semantic solution reduces errors in the data, then measuring the number of errors corrected within the new system can be used to understand the time-savings now that errors do not need to be manually corrected. Similarly, the number of documents auto-tagged can be used to tell a story of time not spent manually tagging content. In the example cited in the two previous sections, after cleaning up content and applying the new taxonomy through auto-tagging EK estimated that the necessary human effort related to the content migration was reduced by nearly 80%. This is truly a remarkable improvement that had both immediate and long term benefits.\n\n\n\nConclusion\n\nThroughout our engagements EK has learned critical lessons about the importance of creating and using KPIs.  Anecdotal evidence can be a powerful tool, but even more powerful are anecdotes backed by quantified measures. Learning how to set and track KPIs at the outset of a project provides the opportunity to understand and quantify the impact of semantic integrations. Across a variety of projects, KPIs have been core data points that help keep projects focused, and keep efforts directed towards the most important areas of a project.  When faced with murky or unclear objectives and a lack of KPIs our teams have seen projects sit with the wheels spinning, have needed to re-do work, and have seen focus shift onto unnecessary components of a project, to the detriment of other components. On the flip side, well crafted KPIs have been utilized to prove business value, spot inefficiencies, and improve semantic solutions over and over again. KPIs can be utilized over time to tell a story that will help you and your business better understand and track goals met through the use of your taxonomy, ontology, or knowledge graph. The team at EK is well versed in developing both the semantic solutions you need as well as the KPIs necessary to evaluate, understand, and promote these solutions. To learn more about how we can help, contact EK today ."
    },
    {
        "title": "Knowledge Cast – Knowledge Summit Dublin 2024",
        "url": "https://enterprise-knowledge.com/knowledge-cast-knowledge-summit-dublin-2024/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "ai",
            "knowledge management"
        ],
        "article_type": "podcast",
        "content": "In this episode of Knowledge Cast, Enterprise Knowledge CEO Zach Wahl speaks with guests Barry Byrne, Darryl Wing, Scott Leeb, and Bill Kaplan about Knowledge Summit Dublin 2024.\n\nIn this discussion, the group speaks about the creation of the event, what they will be speaking about during the conference, the future of generative AI, and what they expect to learn while attending.\n\nTo learn more about Knowledge Summit, and register for the event, go to ⁠ https://www.knowledgesummitdublin.com/ ⁠\n\n﻿ ﻿ ﻿ ﻿ ﻿\n\n\n\n\n\n\n\n\n\nIf you would like to be a guest on Knowledge Cast, contact Enterprise Knowledge for more information."
    },
    {
        "title": "KM Trends – Semantic Layer",
        "url": "https://enterprise-knowledge.com/km-trends-semantic-layer/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "ai",
            "knowledge management"
        ],
        "article_type": "podcast",
        "content": "In this episode of Knowledge Cast, CEO Zach Wahl is joined by his colleagues Joe Hilger, Lulit Tesfaye, and Ashleigh Faith as they discuss the Semantic Layer in the context of knowledge management.\n\nThey define a Semantic Layer and explore how it can enhance knowledge graphs, taxonomies, and content management, focusing on client feedback, industry surveys, and expert interviews and how they can provide valuable insights into this innovative approach to managing information. Tune in to learn more about the potential of a Semantic Layer in knowledge management!\n\n﻿ ﻿ ﻿ ﻿ ﻿\n\n\n\n\n\n\n\n\n\nIf you would like to be a guest on Knowledge Cast, contact Enterprise Knowledge for more information."
    },
    {
        "title": "A Structured Content Model and Multi-Channel Publishing for Rapid Content Distribution",
        "url": "https://enterprise-knowledge.com/a-structured-content-model-and-multi-channel-publishing-for-rapid-content-distribution/",
        "categories": [
            "advanced content",
            "software development"
        ],
        "tags": [
            "advanced content",
            "componentized content",
            "multichannel content management",
            "omnichannel"
        ],
        "article_type": "case study",
        "content": "The Challenge\n\nEK partnered with the office of a large government agency whose primary mission required them to rapidly distribute real-time updates about current events to government executives and their staff. At the same time, they needed the ability to manage the knowledge inherent in these real-time updates for later analysis. The requirements of rapid and near-instantaneous distribution of content pulled in opposition to the requirement for more structured content which would enable analysis. Their old system, which involved drafting content in OneNote and pasting directly into Outlook to send email updates, provided no efficient way to view or leverage previously published content, nor did it allow them to save and reuse content or content templates. The office needed a content management and distribution system that allowed them to rapidly author, distribute content to multiple publication channels, and retrieve and analyze previously published content to inform policy decisions.\n\nThe Solution\n\nEK architects, developers, and strategists worked with the office over a period of three years to iteratively develop and release a structured content management system that would revolutionize the way the office managed content. In the first year of the engagement, EK’s expert content engineers performed a detailed content analysis to identify the multi-channel publishing opportunity of content. The output of the content analysis was a content model which defined how content would be broken down into its component parts in the new system, documenting the relationships between content types, supporting metadata, and the overall structure of the content (and templates) which developers would later implement in the content management system.\n\nA typical approach for achieving structured content in a CMS employs the use of predefined fields which are filled out at the time of content creation – similar to the experience of filling out a web form. Due to the need for near-instantaneous content publishing, that experience was undesirable. Content authors needed an authoring experience which felt unstructured, while it actually structured content behind the scenes. The solution for this was a custom built document editor that allowed users to add content in a seemingly unstructured fashion. Upon save, a translation microservice would save the editor’s content into JSON structured content when saved to the database.\n\nGovernment executives who were the ultimate audiences for this content had very strong preferences for how they wanted to consume the content. Some wanted to continue to receive email, some wanted the email to go to an assistant who would print a PDF for them to read while in transit, and some had a strong preference for a mobile app or SMS. By structuring content, we enable the flexibility to design multiple end-user experiences to consume the content.\n\nWhile the need for immediate distribution to government executives was met through these distribution channels, analysts had a need to search for previously published content in order to analyze trends and make policy recommendations. This type of deeper analysis is better supported by a web interface and so a web application with search functionality became yet another publication endpoint.\n\nOne thing is a constant of content models:  as an organization evolves, so must the model. Indeed, over the course of our three year engagement, the name and structure of the reports being authored by the government office changed several times. It became imperative that the custom CMS EK developed would support the administrative management of additional content structures and templates. In response to this need, EK created a template manager which allowed non-technical content designers to define their own templates. They could instead choose which structured content components from the content model would be in the default content creation experience as a content author rapidly created a new piece of content directly through the administrative interface.\n\nThe EK Difference\n\nEK employed our Advanced Content expertise to design and develop a custom CMS which met the complex requirements of the government agency. Our expert consultants identified the need for rapid, multi-channel publishing and immediately translated that to the technical requirement of a structured content model and a CMS which would support not only structured content, but also the creation of content in a way which felt unstructured to content authors.\n\nEK conducted a targeted content analysis, facilitating focus groups with the stakeholders to identify themes and duplicate content across publications. This enabled us to help the office define the major and minor content types within publications, highlighting the structured content components that were critical to a publication and had potential for reuse in multiple publications. Additionally, we were able to define the structured content model, including the content types, the structural components which were reusable across content types and end-user experiences, and the metadata which supported reuse of structured content components.\n\nEK not only provided content strategists and content engineers for the engagement, but also provided a solution architect and software engineering team with dedicated CMS development experience. This team was able to customize a Drupal CMS and enrich it with a set of Python microservices in order to maximize the value of the open source tool, but also meet the complex, custom requirements of the government agency.\n\nThe Results\n\nEK architected the content model to accomplish a few different business goals:\n\nReady to Get Started?"
    },
    {
        "title": "Improving Customer Experience in a Personalized Customer Resource Portal",
        "url": "https://enterprise-knowledge.com/improving-customer-experience-in-a-personalized-customer-resource-portal/",
        "categories": [
            "advanced content"
        ],
        "tags": [
            "advanced content",
            "content personalization",
            "customer experience",
            "resource portal"
        ],
        "article_type": "case study",
        "content": "The Challenge\n\nThe customer support team of a global corporation recently undertook an initiative to transform the content in their Customer Resource Portal from unstructured, long-form PDFs to structured content in a DITA based Component Content Management System (CCMS). This Customer Resource Portal contains detailed manuals and information about the hundreds of products and services this corporation offers across the globe, many of which are specific to regions or countries. Customers of this corporation rely on the customer service portal to locate specific information about products and services available to them in their location.\n\nWhen they approached EK, the organization was struggling to achieve ROI from this massive undertaking and customer experience (CX) remained stubbornly unsatisfactory. The poor CX of the system resulted in an 8% drop in daily user access and one in five customers reporting that they didn’t use the system at all. Additionally, half of users felt that the search functionality of the system was unsatisfactory. To remediate this, the organization created a complex, multi-year plan to optimize customer experience through the display of dynamic content updates in the front end of the portal, however when EK conducted a technical assessment, our technical consultants found that the off-the-shelf product in use for the presentation layer was not adequate to meet the robust needs of the structured content.\n\nFurther, the content model employed in the CCMS did not adequately enrich the technical documentation content with metadata suitable to enable personalization. Users struggled to find the information as search terms did not always align to query strings in the content. Content and search results were not personalized to the needs of the user, and search queries returned large amounts of information that was not relevant. For the internal content team, tagging and authoring content was a manual effort. Authors were spending too much time adding tags and metadata to documentation, rather than publishing fresh information to the portal. Furthermore, content that was not properly manually tagged became especially difficult to find within the sheer volume of content in the portal.\n\nThe Solution\n\nEK worked with the customer care team at this corporation to architect and implement content personalization improvements to the customer portal. This work began with an assessment and redesign of the key taxonomies needed to better personalize the content in the portal. EK’s taxonomy experts worked with SMEs at the corporation to create intuitive taxonomies for products, services, and user entitlements that best reflected the current state of the business. Then, EK developers and taxonomists worked together to execute bulk tagging of prioritized existing content in the portal with the newly updated taxonomies. Additionally, EK developed a custom auto-tagging authoring tool so that content authors could quickly and easily apply tags to new content being published in the portal. To support the new taxonomies and tagging processes, EK taxonomists drafted a Taxonomy and Tagging Governance Plan to facilitate the long-term, ongoing maintenance of metadata within the system.\n\nDuring this time EK architects and semantic specialists were also helping the corporation steward the implementation of their newly acquired Taxonomy and Ontology Management Tool. This work involved designing roles for the tool, providing an orientation on how to use the tool, uploading the new taxonomies into the system, and providing an integration plan for how the tool should integrate with the customer service portal and its component technologies. This work set the groundwork for the corporation to be able to manage the taxonomies that would underpin personalization from a centralized repository, and then push updates to the systems that leverage them.\n\nOnce taxonomies had been updated, stored in the correct systems, and applied to content, EK architected a solution that would personalize search results relevant to a user’s entitlements and geographical location. The customer service portal boosts search results that are tagged with products, services, and other tags that are related to the user’s profile.\n\nThe EK Difference\n\nEK’s experts in taxonomy, content, and technical implementation came together to provide a comprehensive, innovative content personalization solution for the customer support team. Because EK employs experts in semantic technologies, EK was able to provide expertise and support for the corporation as they were implementing their new taxonomy tool, including providing training for their staff on how to use it. EK technology experts also partnered with the corporation’s technology teams to ensure there was alignment on and understanding of the personalization solution and its components across all teams that would be interacting with or managing it.\n\nEK solution architects and content engineers assessed the corporation’s technology stack on its fitness to support advanced content functionality and found that the current configuration and technologies were inadequate. To address this, EK assessed both internal and external tools available to the corporation and provided them with an architecture that would meet their content needs. This included the vetting of a new front end tool that would significantly improve both CX and search experience for users. With the new architecture in hand, technology leadership could make informed, confident decisions about their future content technology plans and investments.\n\nThe Results\n\nEK was able to provide the customer support team with a sustainable solution for personalizing content in their customer service portal. Currently published content in the system is now tagged for search boosting, and content authors have an intuitive and quick way to tag newly created content. Improvements made by the EK team have rendered promising user experience outcomes, including reduced confusion and customer service inquiries by having 100% of user entitlements aligned with product taxonomies, and by removing 43 outdated or irrelevant terms. The personalized search boosting also increased click through rates of search results by 13%, indicating that users are being served content that is more relevant to them. Additionally, content authors will save approximately 125 hours a year leveraging the new, faster tagging process instead of manually tagging content.\n\nReady to Get Started?"
    },
    {
        "title": "Generative AI-Assisted Taxonomy Development for a Global Investment Bank",
        "url": "https://enterprise-knowledge.com/generative-ai-assisted-taxonomy-development-for-a-global-investment-bank/",
        "categories": [
            "ai",
            "taxonomy ontology design"
        ],
        "tags": [
            "ai",
            "bank",
            "financial",
            "taxonomy"
        ],
        "article_type": "case study",
        "content": "The Challenge\n\nA multinational financial institution with a century-long legacy, celebrated for pioneering financial solutions and shaping the global economic landscape, relied on unstructured data for risk management. With a vast array of risks to consider, and a wealth of insights generated by risk analysts, relying on text-based risk descriptions that proved to be limited in creating consistent reporting and tracking of risk over time.\n\nThe risk analysis process consisted of risk assessors, employing natural language to articulate risks as they were identified and reported. However, due to the diverse vocabularies and writing styles prevalent across various departments and geographies, the utilization of these risk descriptions posed a considerable challenge in reusability and making them machine readable. Even when integrated with existing taxonomies, the varying language hindered the efficient aggregation and analysis of non-financial risks on a global scale. Using different vocabularies and taxonomies led to information silos, a lack of shared understanding across departments and products, and an overallocation of resources to analyze and extract valuable conclusions from texts that seemed to be very different.\n\nThe firm recognized the need for a more structured and uniform approach to risk assessment. The solution required the implementation of a new taxonomy and risk classification system to improve risk management practices, enhance decision-making, and optimize resource allocation. The new taxonomy and risk classification system would give the company a holistic view of risks and facilitate data-driven strategic decisions.\n\nThe Solution\n\nTo address this challenge, EK leveraged an agile process using generative AI (Gen AI) to expand and refine the client’s risk taxonomy. The goal was to move beyond free-text descriptions, enabling the financial institution to aggregate risks accurately for analytical purposes and ensure a comprehensive overview of their non-financial risk landscape. The solution contained the following components:\n\nThe EK Difference\n\nThe AI-assisted taxonomy development is one component of EK’s comprehensive transformation and enhancement of the risk data management environment for the firm. Bringing unparalleled expertise to the program, our team plays a pivotal role in guiding the transformation of risk data from current to future states. By providing strategic insights into data management, semantic technologies, AI models, and architecture development, we lay the foundation for creating semantic models that drive risk-reporting technology within the program.\n\nOne of our distinctive strengths lies in the consistent advisory and expert guidance we provide in semantic modeling for risks. This includes case-driven ontology modeling, enabling semi-automated and semantically enhanced risk assessment. Our quarterly releases of evolving semantic structures and enabling technologies align with the dynamic goals of the program, ensuring it stays at the forefront of technological advancements and risk management best practices. Simultaneously, our team offers architectural recommendations to connect, mature, and integrate technologies, fostering a robust future-state ecosystem.\n\nFurthermore, the EK team leads in blending the firm’s diverse taxonomies and controlled vocabularies related to risk assessment. Our role is to lead forward-thinking and advanced data science techniques to move the program forward and continuously improve the firm’s processes at scale. We focus on creating a unified model from dozens of siloed vocabularies, ensuring streamlined and consistent reporting capabilities. Retaining historical mappings is crucial, allowing us to provide stakeholders with historical metrics and consistency in risk reporting processes. As an innovator in risk management, the EK team leverages advanced data science techniques, as exemplified by our engagement, employing semantic similarity algorithms and large language models to optimize the program’s future state of risk categories.\n\nThe Results\n\nLeveraging an agile process, EK employed generative AI to expand and refine the risk taxonomy. This process involved a multi-faceted approach, incorporating topic modeling, generative AI, and prompt engineering. The outcome was a final taxonomy that offered a profound understanding of the interconnectedness of risks over time and across divisions, allowing the firm to assess better their potential impact on the organization on a global scale.\n\nThe human-in-the-loop taxonomy development further elevated the model-generated risk topics through expert reviews and categorizations. This process enabled the aggregation of about 10,000 yearly records into approximately 1,500 categories from the newly developed risk taxonomy. This aggregation reduced the time it took risk SMEs to review and categorize risk incidents from 45-50 minutes to 1-2 minutes per risk occurrence.\n\nAt EK, we specialize in transforming complex challenges into streamlined processes. By leveraging Generative AI, we revolutionized our client’s risk management use case, automating taxonomy development processes and providing an understanding of interconnected risks. Our demonstrated approach, combining advanced technology with human expertise through human-in-the-loop models, ensures not just automation but optimization, allowing your team to focus on strategic decision-making. Contact us for generative AI applications that can support your organization in handling your data complexities more efficiently and with improved foresight.\n\nReady to Get Started?"
    },
    {
        "title": "What is a Semantic Layer? (Components and Enterprise Applications)",
        "url": "https://enterprise-knowledge.com/what-is-a-semantic-layer-components-and-enterprise-applications/",
        "categories": [
            "knowledge graphs data modeling"
        ],
        "tags": [
            "applications",
            "components",
            "semantic layer"
        ],
        "article_type": "blog",
        "content": "Over the last decade, many organizations went through expensive migrations – either moving data into a data lake, a data warehouse, a modern data stack, or to the cloud. Yet, the business problems that many are looking to solve through these transformation initiatives still persist, including:\n\nSo, what is a semantic layer and how does it address these challenges? Back in 2020, I first discussed a Semantic Layer through a white paper I published, What is a Semantic Architecture and How do I Build One? . In 2021, Gartner dubbed it “a data fabric/data mesh architecture and key to modernizing enterprise data management.” As the field continues to evolve and technical capabilities advance with developments in data and AI solutions, I have broken down the definition based on this fast-paced industry maturity to reflect the latest developments.\n\n\n\nWhat is a Semantic Layer?\n\nA semantic layer is a standardized framework that organizes and abstracts organizational data (structured, unstructured, semi-structured) and serves as a data connector for data and knowledge. Larger than a data fabric, that is more focused on structured data, a semantic layer connects all organizational knowledge assets including content items, files, videos, media, etc. via a well defined and standardized semantic framework. It allows organizations to represent organizational knowledge and domain meaning to systems and applications, defining the relationship between content and data. Specifically, a semantic layer:\n\nA Semantic Layer is the culmination of a noticeable shift in business focus and realization that business insights are not gained from having data physically co-located in one place (like a data lake) but by understanding the meaning of data within an organization’s context and how it is related.\n\n\n\n\n\nWhat are the Components of a Semantic Layer?\n\nA semantic layer is not a single platform or application, but rather the actualization of a semantic approach to solving business problems by managing data in a manner that is optimized for capturing business meaning and context – and designing it for end user experience. A scalable semantic layer includes one or more of the following components to build a viable solution framework for today’s enterprise.\n\n1. Metadata\n\nThe most effective way to make datasets easier to organize, understand, and manage is to enhance them with rich and descriptive data, i.e., metadata. Metadata plays a key role in a semantic layer by providing essential information and context about the underlying data. This includes establishing a shared approach to provide the organization with information about data sources, standardized data, relationships between data elements, security and access controls, versioning, lineage, data quality and governance measures, and other relevant details to drive efficient labeling and categorization.\n\n2. Taxonomy & Information Architecture\n\nBusiness taxonomies allow us to describe, align, and represent organizational vocabulary in a structured format (through hierarchies), complementing metadata by providing an additional layer of organization. Taxonomy plays a crucial role in a semantic layer by ensuring consistent naming conventions and classification standards, reducing ambiguity and promoting a shared understanding of business concepts. The primary use case for many of our clients is to design taxonomies to be cross-functional so they can be applied across different departments and business units and ultimately facilitate data discovery and exploration of shared data through faceting. As such, taxonomies and information architecture promote a standardized approach to information and data management/governance practices and provide structured business context for the semantic layer to keep up with evolving business environments, processes, and terminologies.\n\n3. Business Glossary\n\nOne of my favorite quotes from Socrates is, “The beginning of wisdom is the definition of terms.” And yes, business alignment is all about semantics! This quote highlights the importance of clearly defining terms, a principle that aligns with the purpose of a business glossary in establishing shared meanings within a business context. Within our context of data and knowledge management, a business glossary would ideally be part of the ontology and/or taxonomy and aligns business with technical understanding and serves as one of the most common components of a semantic layer by facilitating effective communication across the organization, and its systems.\n\n4. Ontology\n\nA flexible data model/schema structure shifts the focus of traditional/tabular data solutions from the data itself to the relationships of data elements and their meaning. The role of ontology in a semantic layer is to provide a formal representation of the knowledge and relationships within a specific domain or subject area. This includes the creation of entities, attributes, and relationships that reflect the business concepts. As such, ontology goes beyond taxonomy and metadata by capturing not only the hierarchical structure of data but also the semantics and meaning of the relationships between different data concepts. Much like how a blueprint defines the structure, relationships, and purpose of each room in a building, ontology provides a logical schema to define the structure, relationships, and meaning of data in a system, enabling a clear and organized understanding of data that is typically related but siloed.\n\n5. Knowledge Graph\n\nFor specific use cases, a knowledge graph is created when business concepts and defined relationships from ontological schemas are applied to data/content. A knowledge graph plays a significant role in a semantic layer by representing information as interconnected entities and relationships, providing a structured and graph-based approach to knowledge representation.\n\nA knowledge graph allows organizations to connect heterogeneous data sources by linking entities and relationships across different datasets and to store business rules and logic with data and to transform raw data into meaningful information. A knowledge graph further aligns well with the principles of Linked Data , where entities are connected through links, creating a web of interconnected data. Examples of use cases for graph creation include the need to traverse relationships, apply calculations, aggregations, and other connections or manipulations that align with the business requirements on raw data. For example, one of our clients, a leading firm in global private equity, created a knowledge portal based on a semantic layer that gives them access to all of the key information about their most important business assets, such as deals, investments, bankers, partners, and employees. From a single application, business leaders can see information about these important assets pulled from over 20 different sources (connected by a knowledge graph). A director in the firm can look up an investment to see how it is doing, then view the employee who worked on the original deal, and then see all the other deals they worked on from a single location. Information is organized, not by the systems from which they originate, but by the business asset that the director is viewing. Their leaders now have better access to information and a more natural way to see how their business is performing.\n\n\n\n\n\nWhat are the Applications and Use Cases for a Semantic Layer?\n\nThe primary role of a semantic layer is to simplify the interaction between users and disparate data sources. Similar to how an index aggregates and streamlines the search for relevant content in a book, a semantic layer abstracts the underlying complexity of enterprise data using consistent, standardized, and well defined metadata, without the need to move or migrate physical data from its source. It addresses traditional data management challenges by providing a standardized representation of data elements, making it simpler for users across the organization to access and understand organization’s data regardless of type, size, location and/or department.\n\n\n\n\n\n\n\nSolutions and Sample Architecture\n\nThe specific tools and solutions to architect a semantic layer depend on the organization’s requirements, data governance maturity, and technologies in use. Although the market is continuously evolving and there are many tools currently emerging that purport to provide a semantic layer, the following solutions that provide the capabilities to manage semantics and context make up the building blocks of a scalable semantic architecture. In most cases, we find that these solutions already exist in-house for a majority of the organizations we work with and only require the right architecture and data model to build a usable semantic layer.\n\n\n\n\n\n\n\nClosing\n\nThe evolution and maturity of the semantic layer is a testament to its importance to knowledge and data management. As organizations take on more complex use cases and adopt AI initiatives, the idea of working within one monolithic platform is becoming a thing of the past. Enterprise solutions are looking for ways to abstract their data in a system/application agnostic way in order to be able to work with systems of today and anticipate the solutions of tomorrow.\n\nAs a result, a semantic layer is gaining more adoption and allowing organizations to create that shared standard and interoperability. Additionally, a semantic layer enriches data representation by modeling complex relationships and providing a powerful framework for understanding and exploring interconnected knowledge. It enhances the capabilities of knowledge and content management as well as business intelligence and analytics teams, supporting advanced data analysis, discovery, modeling and decision-making on connected data.\n\nWhen embarking on a semantic layer initiative, not understanding or planning for one or all of the core components and solutions discussed here is what often stalls projects and creates challenges or points of failure for many organizations. If you are looking to get started and learn more about how other organizations are approaching scale – read more from our case studies or contact us if you have specific questions."
    },
    {
        "title": "Expert Analysis: Top 5 Considerations When Building a Modern Knowledge Portal",
        "url": "https://enterprise-knowledge.com/expert-analysis-top-5-considerations-when-building-a-modern-knowledge-portal/",
        "categories": [
            "enterprise search",
            "knowledge graphs data modeling",
            "taxonomy ontology design",
            "software development"
        ],
        "tags": [
            "knowledge portals"
        ],
        "article_type": "blog",
        "content": "Knowledge Portals aggregate and present various types of content – including unstructured content, structured data, and connections to people and enterprise resources. This facilitates the creation of new knowledge and discovery of existing information.\n\nThe following article highlights five key factors that design and implementation teams should consider when building a Knowledge Portal for their organizations.\n\n\n\nSources of Truth\n\nGuillermo Galdamez\n\nWe define ‘sources of truth’ as the various systems responsible for generating data, recording transactions, or storing key documents about the vital business processes of an organization. These systems are fundamental to the day-to-day operations and long-term strategic objectives of the business.\n\nIn a modern enterprise, the systems supporting diverse business processes can number in the dozens, if not hundreds, depending on the organization’s size. However, from the business perspective of a Knowledge Portal implementation, it is critical to prioritize integrations with each source based on appropriate criteria. Drawing from our experience, we’ve identified three key factors that Knowledge Portal leaders should consider:\n\nKate Erfle\n\nA Knowledge Portal should draw from well-defined information sources that are recognized as being authoritative and trusted. A Knowledge Portal isn’t intended to act as the “source of truth” itself, but rather to aggregate and meaningfully connect data sources and repositories, providing a cohesive “view of truth.”\n\nAs Guillermo pointed out, there are several key data and technical readiness factors to consider when integrating source systems within a Knowledge Portal ecosystem . For a successful implementation, the source systems should meet the following technical criteria:\n\nOnce a data source meets the established criteria for quality, import/export capabilities, and security, it becomes eligible for integration with the Knowledge Portal. Within the portal, it may be possible to create or update content, but the data source remains its own “source of truth”. All changes made within the Knowledge Portal should be reflected in the corresponding source system to maintain consistency, accuracy, and integrity of the source system data. During the design and implementation of a Knowledge Portal, it is critical to consider the impact of user actions and to ensure that any changes are accurately reflected in the source data. This approach ensures the continued accuracy and reliability of data from the source systems.\n\nInformation Quality\n\n\n\n\n\nGuillermo Galdamez\n\nOne of the most common issues I encounter when talking to our clients is the perception that their data and unstructured content is unreliable . This could be due to various issues: the data might be incomplete, duplicative, outdated, or just plain wrong . As a result, employees can spend hours not only searching for information and data but also tracking down people who can confirm its reliability and usability.\n\nIn discussing content and data quality, one of the foundational steps is taking inventory of the ‘ stuff ’ contained within your prioritized sources of truth. Though the maxim “You can’t manage what you can’t measure,” has often sparked debate about its merits, this is one occasion where it is notably relevant. It is important for the implementation team, as well as the business to have visibility of the data and content it means to ingest and display through the Knowledge Portal. Performing a content analysis is key in providing the Knowledge Portal team with the information they need to ensure that information provided by the Knowledge Portal is consistent, reliable, and timely .\n\nA content inventory and audit often reveals areas where data and content needs to be remediated, migrated, archived, or disposed of. The Knowledge Portal team should take this opportunity to perform data and content cleanup. During development, the Knowledge Portal Team can collaborate with various teams to improve data and content quality. Even following its launch, the Portal, by aggregating and presenting information in new ways, can reveal gaps or inconsistencies across its sources. It will be helpful to define feedback mechanisms between users, the Knowledge Portal Team, and data and content owners to be able to address instances where data and content needs to be maintained.\n\nGaining and sustaining user trust is crucial for Knowledge Portals. Users will continuously visit the Portal as long as they perceive that it solves their previous challenges. If the Portal becomes a new ‘junk drawer’ for data, engagement will decline rapidly. To avoid this, implement a strong change management and communications strategy to continually remind users about the Portal’s capabilities and value.\n\nKate Erfle\n\nMaintaining high-quality data and content is crucial for a Knowledge Portal’s success. As Guillermo stated, the implementation phase of a Knowledge Portal offers the perfect opportunity for data cleanup.\n\nTo begin, it’s important for individual system owners and administrators to do what is feasible within their systems to ensure high-quality data. Before it’s provided to the Portal, several transformation and cleaning steps can be applied directly to the source system data. The Knowledge Portal implementation team should collaborate closely with the various data repository teams to ensure the required data fields are standardized, cleaned, and validated before being exported. By working together, these teams can assess the current state of the data, identify missing fields, spot discrepancies, and address inconsistencies.\n\nIf the data from source systems still contains imperfections, a few remediation strategies can be applied to prepare it for integration:\n\nGuillermo emphasized the importance of remedying data issues to build and maintain user trust and buy-in. Effectively addressing bad data is also critical to avoid significant issues:\n\nBusiness Meaning and Context\n\n\n\n\n\nGuillermo Galdamez\n\nAs mentioned earlier, Knowledge Portals aggregate information from diverse sources and present it to users, introducing a new capability to the organization. It’s essential for the Knowledge Portal team to fully understand the data and information being presented to the users. This includes knowing its significance and business value, its origin, how it is generated, and its connection to other business processes. Keep in mind that this information is seldom presented to users all at once, so they will likely face a learning curve to utilize the Knowledge Portal effectively. This challenge can be mitigated through thoughtful design, change management, training, and communication.\n\nDesigns for a Knowledge Portal need to strategically organize different information elements. This involves not only prioritizing these elements based on relative importance, but also ensuring they align with business logic and are linked to related data, information, and people. In other words, the design needs to be understandable to all intended users at a single glance. Achieving this requires clear, prioritized use cases tailored to the Knowledge Portal’s audiences , combined with thorough user research that informs user expectations. Knowing this, it becomes easier to design with user needs and objectives in mind and have it more seamlessly fit into their daily workflows and activities.\n\nEffective change management, training, and communications help reinforce the purpose and the value of a Knowledge Portal, which might not always be intuitive to everyone across the organization; some users may be resistant to change, preferring to stick to familiar routines. It’s crucial for the Knowledge Portal team to understand these users’ motivations, their hesitations, and what they value. Clearly articulating the individual benefits users will gain from the Portal, setting clear expectations, and providing guidance on using the Portal successfully are crucial for new users to adopt it and appreciate its value in their work.\n\nKate Erfle\n\nIt is essential to provide context to the information available on the portal, especially within a specific business or industry setting. This involves adding metadata, descriptions, and categorizations to data, allowing siloed, disconnected information to be associated and helping users discover content relevant to their needs quickly and efficiently.\n\nA robust metadata system and a well-defined taxonomy can aid in organizing and presenting content in a meaningful way. It’s important to evaluate the current state of existing taxonomies and controlled vocabularies across each source system, as well as to assess the prevalence and consistency of metadata applied to content within these systems. These evaluations help determine the level of effort required to standardize and connect content effectively. To obtain the full benefits of a Knowledge Portal–creating an Enterprise 360 view of the organization’s assets, knowledge, and data–this content needs to be well-defined, categorized, and described.\n\nSecurity and Governance\n\n\n\nGuillermo Galdamez\n\nOne of the most common motivations driving the implementation of Knowledge Portals is the user’s need to quickly find specific information required for their work. However, users often overlook the equally important aspect of securing this information.\n\nOften, information is shared through unsecured channels like email, chat, or other common communication methods at users’ disposal. This approach places the responsibility entirely on the sender to ascertain and decide if a recipient is authorized to receive the information. Sometimes senders mistakenly send information to the wrong person, or they may need additional time to verify the recipient’s access rights. Furthermore, senders may need to redact parts of the information that the recipient isn’t permitted to see, which adds another time-consuming step.\n\nThe Knowledge Portal implementation must address this organizational challenge. At times, the Knowledge Portal team will need to guide the organization in establishing a clear framework for access control. This is especially necessary when the Knowledge Portal creates new types of information and data by aggregating, repackaging, and delivering them to users.\n\nKate Erfle\n\nSecurity and governance are paramount in the construction of a Knowledge Portal. They profoundly influence various implementation details and are critical for ensuring the confidentiality, integrity, and availability of information within the portal.\n\nThe first major piece of security and governance is user authentication, which involves verifying a user’s identity. Several options for implementing user authentication include traditional username and password, Multi-Factor Authentication (MFA), and Single Sign-On (SSO). These choices will be influenced by the existing authentication and identity management systems in use within the client organization. Solidifying these design decisions early in the architecting process is critical as they affect many facets of the portal’s implementation.\n\nThe second major piece of security and governance is user authorization, which involves granting users permission to access specific resources based on their identity, as established through user authentication. Multiple authorization models may be necessary based on the level of fine-grained access control required. Popular models include:\n\nDepending on the organization’s use case, one or a combination of these may be used to manage user access and ensure sensitive data is secured. The difficulty and complexity of the implementation will be directly correlated with the current and target state of identity and security management across the organization, as well as the breadth and depth of data classification applied to the organization’s data.\n\nInformation Seeking and Action\n\n\n\n\n\nGuillermo Galdamez\n\nKnowledge Portal users will approach their quest for information in a variety of ways . Users may prefer to browse through content during exploratory sessions, or they may leverage search when they know precisely what they need. Often, users employ a combination of these approaches depending on their specific needs for data or content.\n\nFor instance, in a recent Knowledge Portal project, our user research revealed that individuals rarely searched for documents directly. Instead, they searched for various business entities and then browsed through related documents. This prompted the team to reevaluate the prioritization of documents in search results and the necessary data points that should be displayed alongside these documents to provide meaningful context .\n\nIn summary, having a strong user research strategy is essential to understand what type of data and information users are seeking, their reasons for needing it, their subsequent use of it, and how this supports the broader organization’s processes and objectives.\n\nKate Erfle\n\nKnowledge Portals are designed to provide users with access to a broader range of information and resources than available in the various source systems, and they should facilitate users in both finding necessary information and taking meaningful actions based on that information.\n\nInformation Seeking Involves:\n\nAction Involves:\n\nClosing\n\nThe business and technical considerations outlined here are essential for creating a Knowledge Portal that intuitively delivers information to its users. Keep in mind that these considerations are interconnected, and a well-designed Knowledge Portal should strike a balance between them to provide users with a seamless and enriching experience. Should your organization aspire to implement a Knowledge Portal, our team of experts can guide you through these challenges and intricacies, ensuring a successful deployment."
    },
    {
        "title": "Top Knowledge Management Trends – 2024",
        "url": "https://enterprise-knowledge.com/top-knowledge-management-trends-2024/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "1273",
            "artifical intelligence",
            "data trend",
            "km",
            "km trends",
            "knowledge management",
            "knowledge management trends",
            "semantic layer"
        ],
        "article_type": "blog",
        "content": "For the last several years, I’ve written this article on the Top Knowledge Management Trends of the year. As CEO of the world’s largest Knowledge Management consulting company, I’m lucky to get to witness these trends forming each year. As in past years, I brought together EK’s KM consultants and thought leaders to guide the development of this list. I looked at the rising themes we see from our clients and prospective clients, including burgeoning topics in requests for proposals we receive. As we’ve helped many of our leading clients develop multi-year KM Transformation roadmaps, or develop their annual priorities and budgets, key themes have taken shape. I’ve supplemented those insights with a series of interviews from KM leaders and practitioners (both internal and external), reviewed topics and discussions from the world’s KM conferences and publications, and evaluated briefings and product roadmaps from vendors across the KM, information management, content management, and data management software spaces.\n\nYou can review my recent annual blogs for 2023 , 2022 , 2021 , 2020 , and 2019 to get a sense of how the world of KM has rapidly progressed.\n\nThe following are the top KM trends for 2024.\n\nArtificial Intelligence, Obviously – It would be a real failure in thought leadership to simply present AI as a new KM trend and leave it as that, especially since I first identified that growing trend for KM back in 2019. However, the specific interplay and overlap between the two disciplines does merit some additional discussion, as there are new and exciting things happening here.\n\nFirst, organizations are continuing to recognize that their AI initiatives will fail without the appropriate building blocks in place. We’re not talking about black box AI here, but rather explainable AI that can be trusted by even the most risk-averse organizations. In these cases, traditional KM disciplines including knowledge capture and digital communities (to get the expert knowledge in a digital and ingestible form); content structuring (to ensure it is machine readable and configurable); taxonomies, ontologies, and content tagging (to ensure it can be categorized, related, and contextualized); and information governance (to ensure only the correct and appropriate information is utilized by the AI solution) can provide the necessary building blocks to make AI work. None of these KM topics are new, in fact most are decades old, but collectively they can lay the foundation for enterprise AI. With AI as a top executive priority, but many AI initiatives stalled or experiencing early failures, executives are open to revisiting the benefits of KM.\n\nWith AI as a top executive priority, but many AI initiatives stalled or experiencing early failures, executives are open to revisiting the benefits of KM.\n\nThe second KM and AI trend flips the first and focuses on leveraging AI to enhance and improve some traditional KM practices. Over the course of my now quarter-century in KM, there have been several early stumbling blocks to successful KM transformations, largely borne out of the highly labor-intensive nature of KM tasks like content cleanup, content tagging, and content restructuring. These tasks are critical to achieving high KM maturity for an organization, but they can take a massive effort to accomplish effectively. AI offers a solution by automating these and other critical but monotonous KM tasks, speeding up transformations while still delivering a high level of accuracy. This trend promises to drastically improve the speed and/or completion of KM and Digital Transformations.\n\nAs we progress more deeply into the KM/AI trend, there are three primary use cases that I expect will continue. The first, and most common we’ve seen move to production, is customized learning, where AI is being used to automatically assemble individual learning paths. This involves assembling formal and informal learning, access to experts, knowledge assets, and job aids into customized curricula for each individual learner. The second use case is leveraging AI for the assembly and creation of new knowledge articles, combining an organization’s knowledge, content, and data, into newer, richer, and more actionable knowledge assets. I address that in greater detail when speaking about Semantic Layers and Conversational KM later on. The last, and one that I’m particularly excited about, is the use of AI to identify and capture new knowledge from experts. This is not a new idea, but we’re beginning to see investment in this space that will allow solutions to identify risks related to human expertise leaving an organization, as well as the appropriate moments and mechanisms to capture that expertise so it may be preserved within the organization. This is an early trend, but it’s one I think we’ll be seeing a lot more of a year from now.\n\nFocus on KM Doing What AI Can’t – I discuss above how KM can be an enabling factor for AI, and AI can be an accelerator for KM as well. Equally, there has been a lot of discussion about which jobs AI will replace. Though AI will do a lot to facilitate and accelerate KM efforts, the role of the (human) Knowledge Management Expert has never been more important within an organization. Though I have no doubt it will get there in time, AI simply can’t do what we can. To that end, the 2024 KM Trend here is a focus on these key gaps, largely 1) capturing expertise with context and interpretation, ensuring an organization is relying on accurate, current, and trusted information, 2) relating knowledge and facilitating people in ways that will foster collaboration, learning, and innovation, and 3) defining the ontologies and large language models to deliver a digital map of how business is done and the relationships that exist therein. None of these are new skills or topics, but at present they are deeply in demand and are highly valued by mature organizations. The first point will be of particular prominence this year; as organizations are increasingly successful in harnessing their collective knowledge, information, and data, the importance of tacit knowledge capture will surge for many organizations hoping to fill gaps in their organizational intelligence.\n\nIt is important to note that what the generative AI community cutely calls hallucinations are actually extremely problematic for an organization seeking AI. A hallucination is actually either a poorly designed connection, a gap in knowledge, or more likely an incorrect input (as in, old, obsolete, or just plain incorrect information). Knowledge Management professionals should be an organization’s hallucination assassins.\n\nKnowledge Management professionals should be an organization’s hallucination assassins.\n\nContent Structure and Quality – At the beginning of my career, the simple selling point for taxonomies and tagging was adding structure to unstructured information. Now, twenty-five years later, we’re still seeking to enhance our content, but the definition of content has broadened, and the structure we’re seeking is much more mature. The key theme here from the content perspective, as I covered last year, is that KM now covers all forms of content, from tacit to explicit, information to data, and including people, products, and processes all as discrete knowledge assets that can be included as part of an organization’s KM ecosystem. Structure has also progressed from the simple topics of taxonomy and tagging to the design of enterprise-level ontologies, content types, text analytics, and natural language processing to drive not just an understanding of each individual knowledge asset, but the relationships between them and within them.\n\nBuilding the Semantic Layer – The semantic layer is also not a new concept, but it is quickly becoming one of the biggest trends in the overlapping space between KM, Data, and AI. In past years, I’ve written about the trend of Knowledge Graphs and how they’re enabling AI, and now semantic layers are set up to be the next, more powerful, step in that progression. A semantic layer is a standardized framework that organizes and abstracts organizational data (structured, unstructured, semi-structured) and serves as a connector for data and knowledge. It combines many of the core design elements of KM, namely information architecture, taxonomies, ontologies, metadata, and content types, along with traditionally data-centric elements like business glossaries and data catalogs to deliver highly contextualized, integrated knowledge at the point of need. If this is a new term for you, get ready to hear it a lot more. It is more than an enabler for AI, setting organizations up to realize longstanding KM goals of breaking down silos; connecting all forms of data, information, and knowledge with the people who need it; and leveraging analytics to fill gaps in knowledge and performance. In short, it is the solution that may finally deliver true enterprise knowledge for an organization.\n\nRenewed Executive Interest and Openness – I’ve noted in the past that executives were already more open to investment in KM due to the pandemic and subsequent trend toward remote/hybrid work, in addition to the “Great Resignation” and battle for talent. Adding to that, at present, is the massive focus on AI. The key to this trend is that it likely means bigger budgets for knowledge management, but only if you know what to listen for. Executives will be asking for the big AI solution, and they will be more specifically seeking automated content assembly, content cleanup, learning, and knowledge layers. The letters “K” and “M” may not be in the request, but mature KM professionals need to understand the ask and know the central role they play in delivering on it. Put simply, KM’ers can finally be the cool kids, but only if they know how to position KM where it belongs within the organization.\n\nKM’ers can finally be the cool kids, but only if they know how to position KM where it belongs within the organization.\n\nConversational Knowledge Management – We’ve all been amazed by the conversational nature of ChatGPT that allows novice users to ask for answers to questions, images, ideas, and even code, conversing to clarify exactly what you want. As we jointly mature in KM and AI, we’re trending toward “conversational” KM solutions that expand on advanced search, knowledge portals, and intelligent chatbots to allow any user to interact with an organization’s knowledge assets and get increasingly pertinent and customized answers that will help them complete their mission. We’ve already delivered this for some of our more advanced customers, but both the associated technologies and the organizational use cases are hitting a point of inflection where conversational KM capabilities will quickly become the norm.\n\nKM for Risk Identification and Mitigation – Historically, the value of KM has been difficult to express, but we’ve actually made great progress in that area by focusing on the business outcomes of KM, including improved productivity, cost reduction, employee retention, faster and better onboarding and learning, and customer retention, to name a few of the big ones. A new trend in KM comes with a new type of business value: risk identification and mitigation. KM can help identify and mitigate risks by leveraging comprehensive KM solutions to spot improperly secured or incorrect content. This is of particular value for highly regulated industries or those dealing with confidential information. The ROI on this is clear, as one accidental release of proprietary information can cost millions. Worse yet, the wrong information delivered about how to use a product can cost lives. There are other use cases specifically about spotting gaps in knowledge before they become dire, but in short, an enterprise approach to KM can allow an organization to better understand all of their knowledge, content, and data, allowing them to proactively address measurable risks that might occur. This final trend is particularly noteworthy given how easy it is to justify investment, delivering major impact for organizations.\n\nDo you need help understanding and harnessing the value of these trends? Contact us to learn more and get started."
    },
    {
        "title": "Content Audit Workshop",
        "url": "https://enterprise-knowledge.com/content-audit-workshop/",
        "categories": [
            "advanced content",
            "content brand strategy",
            "enterprise learning"
        ],
        "tags": [
            "content",
            "enterprise learning"
        ],
        "article_type": "product",
        "content": "Kick-start your content audit with a practical, skill building workshop\n\nWhether you are planning a content management system implementation, a structured content initiative, a redesign or rebranding, or other content initiative, a content audit is the first step in understanding your current content landscape and preparing for your project. A well-designed and implemented content audit will help ensure that your content meets your business objectives and user goals and is in compliance with your style, discoverability, and management requirements.\n\nOur interactive workshop can be customized to meet the specific needs of your organization, aligning stakeholders across the business to set the foundation and develop a roadmap to accelerate a content improvement initiative.\n\nThe EK Advantage\n\nDownload the Content Audit Workshop Brochure\n\nEK’s content analysis experts will:\n\nWhy Audit\n\nA content audit allows you to\n\nWorkshop Outcomes\n\nBy the conclusion of the workshop you will have an actionable content audit plan including:\n\nReady to get started? Contact us at [email protected]"
    },
    {
        "title": "Content Analysis",
        "url": "https://enterprise-knowledge.com/content-analysis/",
        "categories": [
            "advanced content"
        ],
        "tags": [
            "content analysis"
        ],
        "article_type": "product",
        "content": "Connect your content strategy to your business strategy with a rigorous analysis\n\nYour content isn’t just words on a page; it’s a vital business asset driving your marketing, documentation, communications, customer support, and knowledge management.\n\nBeyond the text, it’s about optimizing the systems that manage and deliver content for optimal usefulness, relevance, and impact. To ensure that your content aligns with current business goals and user needs and stays future-ready, you need to understand how your content is created, structured, and managed, what your users are engaging with, and how that content performs—informing decisions and maximizing the ROI of your content investments.\n\nContent Analysis is the practice and process of analyzing the current state of content and content systems against business objectives and user goals to deliver practical, usable outcomes that enable informed decision-making and support the ROI of content investments.\n\nWhat We Do\n\nDownload the Content Analysis Brochure\n\nThe EK Advantage\n\nWhy Content Analysis?\n\nContent analysis is conducted in support of a variety of content initiatives. The outcomes you’re trying to achieve will drive the factors by which content and content systems are assessed. These initiatives may include:\n\nOutcomes of a Content Analysis\n\nAt the end of a content analysis, you will have\n\nHow Does It Work?\n\nReady to get started? Contact us at [email protected]"
    },
    {
        "title": "Effective Knowledge Management: A Key Success Factor for Distributed Organizations",
        "url": "https://enterprise-knowledge.com/effective-knowledge-management-a-key-success-factor-for-distributed-organizations/",
        "categories": [
            "strategy design"
        ],
        "tags": [],
        "article_type": "blog",
        "content": "In recent years, the distributed workforce model has emerged as a workplace competitive advantage, leveraging technology to engage top talent independent of geography. This approach has many advantages, including access to global talent pools, cost efficiency, and increased organizational flexibility. However, it has also introduced unique challenges. We have heard from our clients that knowledge sharing, information transfer, and collaboration grow increasingly difficult for their employees to do successfully. Quality of information varies widely, as does the ability to find it, slowing down and clouding the decision-making process.\n\nIn this blog, I’ll share with you why effective Knowledge Management (KM) practices play a pivotal role in enabling your distributed organization to thrive. I will also give a preview of how effective KM practices facilitate and successfully ensure the creation, sharing, and utilization of high-quality information and knowledge across dispersed teams. This allows the flow of business to continue smoothly and sets organizations up for success when planning for advanced technologies like Artificial Intelligence (AI).\n\n\n\nEnhanced Collaboration and Communication\n\nAs a leader or member of a distributed organization, one of the primary reasons you may be interested in building and maintaining an effective KM experience is to enhance collaboration and communication. In a distributed environment, team members often work across different time zones, cultures, and languages. Often, knowledge is exchanged on a person-to-person or small network basis; employees have access to information because of who they know or where they’re located. This informal, ad-hoc way of sharing information is not scalable.\n\nEffective, modern approaches to knowledge transfer exist and can be custom-fit to organizational needs. These approaches, especially when supported by content and process governance, ensure that valuable insights, best practices, and lessons learned are captured and consistently accessible to all team members. This fosters a sense of connection, cohesion, and shared purpose, reducing the impact of challenges posed by physical separation and leading to better business outcomes.\n\nWe have seen these benefits firsthand. In one example, the National Park Service (NPS) realized that they needed to connect their volunteers and employees from across the country. EK designed and launched Communities of Practice which connected experts in various NPS topics across the country. The result is a connected workforce who are able to easily find one another to leverage and build on each others’ knowledge.\n\nOne of EK’s clients instituted Communities of Practice to connect volunteers and employees from across the country. This program has helped experts locate and connect with one another for simplified sharing of information.\n\n\n\nKnowledge Capture, Retention, and Learning Culture\n\nMany clients have expressed their concerns about the risk of knowledge loss when employees leave or transition to other roles. Effective KM programs facilitate knowledge retention by capturing both explicit and tacit knowledge , ensuring that critical knowledge and information is preserved and available to current and future employees, ideally at the point-of-need. This can and should be infused into any organization’s learning culture.\n\nCombining the power of enterprise learning and effective KM ensures that the exchange of ideas and information is not static and unidirectional; instead, structured and accurate information is captured easily and flows freely amongst all employees. A frequent misconception is that only new employees benefit from the knowledge of older, more experienced employees, but this simply isn’t the case. A planned, structured approach such as Communities of Practice, solid enterprise search inclusive of collaboration tools such as Teams or Slack, or even Job Shadowing for exchange of information between employees strengthens every employee’s ability to respond to new challenges as employees build upon existing knowledge and develop new strategies and solutions to novel challenges. When captured and stored appropriately, the cycle can begin anew. This is a great application for any learning organization to own and foster.\n\nA planned approach for exchange of information between employees strengthens every employee’s ability to respond to new challenges, allowing employees to build upon existing knowledge and develop new strategies and solutions to novel challenges.\n\n\n\nQuality, Consistency, and Findability of Information\n\nWe also hear that maintaining quality and consistency across processes and procedures has become increasingly challenging for our clients, especially those with dispersed teams. Effective KM helps address this challenge by standardizing processes, documenting procedures, and disseminating guidelines across the organization. The most successful programs have systems and processes that are designed and built to complement the flow of work, not disrupt it. At the same time, ensuring content is accurate and can be trusted is paramount. We recently worked on a Business Management System for a globally distributed client, where critical control process documentation is housed. In this instance, our research allowed us to reasonably estimate that this system could potentially contribute nearly $1.5 million per day in reducing revenue loss through well-structured, accurate, and easily located content, should a mechanical failure occur elsewhere in the organization.\n\nThis value underscores the need for governance of content collections. In this client’s case, they are subject to certain regulatory requirements, but they also have stringent requirements of their own. Their governance model exists to ensure that content is structured and maintained to meet all of these requirements.\n\nWe estimated that one client’s Business Management System, a knowledge repository of critical business control documents for employees around the globe, could potentially contribute nearly $1.5 million per day in reducing revenue loss through well-structured, accurate, and easily located content.\n\n\n\nEfficient Decision-Making\n\nDecisions made within a distributed organization rely heavily on the information available to decision-makers; for example, accurate and specific equipment specifications or manuals easily accessed in the case of an equipment failure. This concept applies equally to executives, who need current and accurate data to make timely and impactful decisions. Effectively applied KM principles provide a structured approach to capturing, organizing, and presenting relevant content and data, which in turn can enable rapid and well-informed decisions. This can minimize delays caused by the need to gather, validate, and cross-check scattered data, reducing the risk of inaccurate decisions caused by faulty or missing data. This level of agility is essential in today’s fast-paced business landscape, where saving a small amount of time on routine tasks can translate into significant cost savings at scale.\n\nFurthermore, as organizations look to the promise of AI to assist in decision-making, it is critical to remember that knowledge-based AI efforts will utilize the content that it has available. If the knowledge content that is available is missing, inaccurate, or duplicative, there is a strong likelihood that the AI results will be consequently skewed.\n\nEffectively applied KM principles provide a structured approach to capturing, organizing, and presenting relevant data, which in turn can enable rapid and well-informed decisions. This becomes critical for organizations considering AI implementations.\n\n\n\nRisk Management and Compliance\n\nEK has worked with a number of heavily regulated clients who have unique needs dictated by diverse regulatory environments that require strict adherence to legal and ethical standards. Effective KM that reflects a structured approach to documenting and disseminating relevant regulations, policies, and procedures can assist these clients in managing risks and ensuring compliance. When supported and promoted by the learning organization, leaders can ensure that this information is proactively presented or provided at the point-of-need. This not only reduces the risk of non-compliance, but it also enhances transparency and accountability across the organization, even in geographically dispersed teams.\n\nThe Business Management System instance discussed earlier is a prime example of how structured and regulated content can be effectively disseminated across broadly distributed organizations. When combined with access management data, this type of content can be specifically delivered to the right users at the right time—based on geographic location, job role or job level, etc. Again, when paired with standard learning organization tools like Workday Learning or other LMS’s, compliance tracking can be more easily monitored and managed.\n\nEffective Knowledge Management reduces the risk of non-compliance, but it also enhances transparency and accountability across the organization, even in geographically dispersed teams.\n\n\n\nConclusion\n\nTo conclude, the importance and value of effective Knowledge Management for distributed organizations cannot be overstated. This organizational model is expected to remain popular, and lack of action in ensuring you are employing effective KM techniques will have far-reaching business impacts .\n\nBy ensuring truly effective KM implementation, distributed organizations can navigate the challenges of their unique environments and unlock the full potential of their diverse teams. This, in turn, will lead to long-term organizational success.\n\nAre you wondering if your distributed organization has a truly effective KM program? Enterprise Knowledge has the experience and tools to fully assess, diagnose, recommend, and implement an effective KM environment in diverse—and distributed—organizations of all sizes. Contact us today to get started."
    },
    {
        "title": "Five Tips for Improving Lessons Learned in Project-Based Organizations",
        "url": "https://enterprise-knowledge.com/five-tips-for-improving-lessons-learned-in-project-based-organizations/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "business processes",
            "engagement",
            "knowledge capture",
            "lessons learned",
            "reuse"
        ],
        "article_type": "blog",
        "content": "The mechanics of completing lessons learned efforts can be deceivingly simple: You get people in a room (physically or otherwise) and discuss opportunities for improvement based on what they experienced during a project. However, many teams and organizations experience difficulty in realizing the full business value they expect from their lessons learned efforts.\n\nFirst, it is helpful to define what a lesson learned is. In a broad sense, a lesson learned is knowledge created over the course of past work that is recalled and applied to improve present and future efforts. For example, a project team overcoming a challenge by adapting an existing process or tool can yield innovations towards future project approaches. Another team may dissect a failure to identify what has gone wrong and enact changes to avoid it in the future. In this blog, I discuss five tips on how to capture and apply lessons learned in your organization.\n\n\n\n\n\nIdentify moments of high-value knowledge capture\n\nMemory can be quite fragile. Details of what occurred as part of a project can get fuzzy quickly, and the more time goes by, the greater the mental effort any one person must spend in retrieving their memories.\n\nMany teams incorporate a lessons learned component at a project’s conclusion through retrospectives or after-action reviews. If your project or initiative spans multiple months, or even years, then waiting until its end to elicit lessons learned runs the risk of missing key details, simply because participants have already forgotten.\n\nThe antidote for loss of key details over time? Identify moments of high-value knowledge capture and incorporate them into your project or sprint plans in advance. If you are working as part of the project, you may not need to wait until the end to begin capturing lessons learned, for every stage of the project represents an opportunity to discuss lessons learned. For instance,  if you have made important decisions, tried out something new, or experienced something that did not go as anticipated, it is important for team members’ memories to be as recent as possible so that they can recall valuable and meaningful details.\n\n\n\n\n\nInclude the right voices in the conversation\n\nIn increasingly complex and distributed work environments, it is rare that any single person can get (or give) a full view of what is happening in a project with all of the interdependencies and interactions that affect the outcome of an initiative. Therefore, discussions can benefit from the holistic perspective that a diverse group can bring.\n\nConsider the team members, stakeholders, and partners that can contribute to these conversations, and invite them to participate in the lessons learned discussion. Moreover, think broadly; these individuals are likely to span across multiple functions and up and down hierarchies. If relevant, bring people in who are external to the organization, such as partners, vendors, consultants, and others who may be able to contribute to a comprehensive understanding of the project. Make the most out of everybody’s time together by defining an agenda and prioritizing specific discussion points beforehand.\n\n\n\n\n\nEngage people meaningfully\n\nAcknowledge participants’ own preferences, habits, and agendas. These will differ widely, especially when dealing with a diverse group. Establish ground rules, expectations, and objectives for a lessons learned session early on, and enforce them. If necessary, do a quick touchpoint with select participants prior to the session to ensure alignment on the objectives and prevent any surprises.\n\nKeep in mind there may be individuals who dominate conversations and others who are naturally quiet. There will be some participants who need to talk out their ideas aloud, while others may need quiet time to reflect. In order to elicit the most powerful nuggets of knowledge out of participants, you may want to leverage a variety of channels: some verbal, some asynchronous, some written. The key here is to adapt your approach to maximize peoples’ contributions. In addition, while a live session may be the best method to maximize engagement, the reality of work is that some people will need to join remotely, and others in different time zones may not be able to join at all. Having a diversity of options for engagement will enable a wider segment of individuals to participate.\n\nThis being said, the purpose of engaging stakeholders in lessons learned is to get a holistic account of how the work was done, what factors influenced the project’s results (whether expected or unexpected), and what can be done so that future projects can perform better. Here is a list of key knowledge nuggets that you should try to elicit:\n\nFinally, conversation should be focused on producing actionable insights. These should consider modifications to the way work is conducted before, during, and after the project concludes. It is helpful if these tasks have an owner, a deadline, and someone to provide guidance and oversight in case people run into delays or roadblocks.\n\n\n\n\n\nSave captured lessons learned for easy retrieval and reuse\n\nPeople may contribute as many ideas, insights, and learnings as they can, but if there is no way to effectively capture lessons learned in a manner that is consistent and retrievable, then these learnings will be difficult to find and apply in the future.\n\nTo capture lessons learned effectively, you need two things:\n\n\n\n\n\nLoop lessons learned back into your business processes\n\nIn my client experiences, I’ve often seen lessons learned become an item in a to-do list; as long as they have a meeting at the end of the project or fill out a simple report, they can claim they do “lessons learned,” but the organization is not meaningfully learning .\n\nAn organization doesn’t actually learn unless it applies the lessons and knowledge it has captured to present and future ways of working. For example, lessons learned may lead to changes in the way projects are staffed, introducing checks and safeguards at different stages of the project, bringing in experts or stakeholders when certain types of decisions are made, or including certain knowledge as part of new employee onboarding.\n\nThere may be several challenges to organizations being able to apply lessons learned. There may be technical considerations in capturing, retrieving, and curating lessons learned within a knowledge base, or the lessons learned may require individuals to change their behaviors and habits.\n\nFrom a technical standpoint, building on having a knowledge base and consistently-tagged information within it, lessons learned can be indexed by a search tool to make them increasingly findable . More advanced applications would include integrating lessons learned into a knowledge graph so that they can be associated with different data and artifacts across the organization, then incorporated into a recommender system that proactively delivers relevant lessons learned to project leaders at key moments throughout the project.\n\nBehavioral challenges can be tricky to overcome as well, and you can take both a top-down and a bottom-up approach to address them. Top-down strategies may include embedding lessons learned retrieval throughout different project stages or prior to key project activities. However, these will need to be complemented through a bottom-up approach: The people receiving new knowledge in the form of lessons learned must be convinced that the knowledge is valuable and that applying it will bring benefits to their work. A holistic change management and communications approach may be necessary to reinforce the expectations around lessons learned, share success stories, and reiterate the value that lessons learned bring to individuals and their teams.\n\n\n\n\n\nClosing\n\nLessons learned, captured and applied effectively to future projects, can bring a bounty of benefits to organizations. The knowledge that is generated as part of everyday work can, and should be, leveraged to improve the results of future work—enhancing the experience for organizations’ customers by learning to better anticipate their needs or prevent mistakes, their employees by better delivering the resources they need to succeed, and their bottom line by achieving efficiencies and introducing innovations.\n\nAt EK, we specialize in all things Knowledge Management, from strategy, to design, onto implementation and maintenance. If you need any help setting up or improving lessons learned, we will be glad to help, no matter where you are on your journey. Contact us today!"
    },
    {
        "title": "The Role of Ontologies with LLMs",
        "url": "https://enterprise-knowledge.com/the-role-of-ontologies-with-llms/",
        "categories": [
            "ai",
            "knowledge graphs data modeling",
            "taxonomy ontology design"
        ],
        "tags": [
            "ai",
            "artifical intelligence",
            "large language model",
            "llm",
            "ontologies"
        ],
        "article_type": "blog",
        "content": "In today’s world, the capabilities of artificial intelligence (AI) and large language models (LLMs) have generated widespread excitement. Recent advancements have made natural language use cases, like chatbots and semantic search, more feasible for organizations. However, many people don’t understand the significant role that ontologies play alongside AI and LLMs. People often ask: do LLMs replace ontologies or complement them? Are ontologies becoming obsolete, or are they still relevant in this rapidly evolving field?\n\nIn this blog, I will explain the continuing importance of ontologies in your organization’s quest for better knowledge retrieval and in augmenting the capabilities of LLMs.\n\nDefining Ontologies and LLMs\n\nLet’s start with quick definitions to ensure we have the same background information.\n\nWhat is an Ontology\n\nAn ontology is a data model that describes a knowledge domain, typically within an organization or particular subject area, and provides context for how different entities are related. For example, an ontology for Enterprise Knowledge could include the following entity types:\n\nThe ontology includes properties about each type, i.e., people’s names and projects’ start and end dates . Additionally, the ontology contains the relationships between types, such as people work on projects, people are experts in tools, and projects are with clients.\n\nOntologies define the model often used in a knowledge graph , the database of real-world things and their connections. For instance, the ontology describes types like people, projects, and client types, and the corresponding knowledge graph would contain actual data, such as information about James Midkiff (Person), who worked on semantic search (Project) for a multinational development bank (Client).\n\nWhat is an LLM\n\nAn LLM is a model trained to understand human sentence structure and meaning. The model can understand text inputs and generate outputs that adhere to correct grammar and language. To briefly describe how an LLM works, LLMs represent text as vectors, known as embeddings. Embeddings act like a numerical fingerprint, uniquely representing each piece of text. The LLM can mathematically compare embeddings of the training set with embeddings from the input text and find similarities to piece together an answer. For example, an LLM can be provided with a large document and asked to summarize it. Since the model can understand the meaning of the large document, transforming it into embeddings, it can easily compile an answer from the provided text.\n\nOrganizations can take advantage of open-source LLMs like Llama2, BLOOM, and BERT, as developing and training custom LLMs can be prohibitively expensive. While utilizing these models, organizations can fine-tune (extend) them with domain-specific information to help the LLM understand the nuances of a particular field. The tuning process is much less expensive to perform and can improve the accuracy of a model’s output.\n\nIntegrating Ontologies and LLMs\n\nWhen an organization begins to utilize LLMs, several common concerns emerge:\n\nThese concerns are all addressed by providing LLMs with methods to integrate information from an organization’s knowledge domain.\n\nFine-tuning with a Knowledge Graph\n\nOntologies model the facts within an organization’s knowledge domain, while a knowledge graph populates these models with actual, factual values. We can leverage these facts to customize and fine-tune the language model to align with the organization’s manner of describing and interconnecting information. This fine-tuning enables the LLM to answer domain-specific questions, accurately identify named entities relevant to the field, and generate language using the organization’s vocabulary.\n\nTraining an LLM with factual information presents challenges similar to those encountered with the original LLM: The training data can become outdated, leading to incomplete or inaccurate responses. To address this, fine-tuning an LLM should be considered a continuous process. Regularly updating the LLM with new and existing relevant information is necessary to maintain up-to-date language usage and factual accuracy. Additionally, it’s essential to diversify the training material fed into the LLM to provide a sample of content in various forms. This involves combining ontology-based facts with varied content and data from the organization’s domain, creating a training set to ensure the LLM is balanced and unbiased toward any specific dataset.\n\nRetrieval Augmented Generation\n\nThe primary method used to avoid stale or incomplete LLM responses is Retrieval Augmented Generation (RAG). RAG is a process that augments the input fed into an LLM with relevant information from an organization’s knowledge domain. Using RAG, an LLM can access information beyond its original training set, utilizing this information to produce more accurate answers. RAG can draw from diverse data sources, including databases, search engines ( semantic or vector search ), and APIs. An additional benefit of RAG is its ability to provide references for the sources used to generate responses.\n\nWe aim to leverage the ontology and knowledge graph to extract facts relevant to the LLM’s input, thereby enhancing the quality of the LLM’s responses. By providing these facts as inputs, the LLM can explicitly understand the relationships within the domain rather than discerning them statistically. Furthermore, feeding the LLM with specific numerical data and other relevant information increases the LLM’s ability to respond to complex queries, including those involving calculations or relating multiple pieces of information. With accurately tailored inputs, the LLM will provide validated, actionable insights rooted in the organization’s data.\n\nFor an example of RAG in action, see the LLM input and response below using a GenAI stack with Neo4j.\n\nConclusion\n\nLLMs are an exciting tool that enable us to effectively interpret and utilize an organization’s knowledge, and quickly access valuable answers and insights. Integrating ontologies and their corresponding knowledge graphs ensures that the LLM accurately uses the language and factual content of an organization’s knowledge domain when generating responses. Are you interested in leveraging your organization’s knowledge with an LLM ? Contact us for more information on how we can get started."
    },
    {
        "title": "EK’s Year in Review – 2023",
        "url": "https://enterprise-knowledge.com/eks-year-in-review-2023/",
        "categories": [
            "company"
        ],
        "tags": [
            "2023 in review",
            "energy",
            "integration",
            "partnership",
            "people",
            "thought leadership",
            "transparency"
        ],
        "article_type": "blog",
        "content": "As we open this new year, I’m happy to have an opportunity to reflect on 2023 and summarize some of EK’s key successes and milestones. For the tenth year in a row, EK grew, adding new customers across the public and private sectors, expanding accounts, and achieving more impact for our clients across an array of services and solutions. We’ve begun 2024 with the largest backlog of signed contracts in the company’s history, actually doubling last year’s previous record, and have just completed a momentous hiring surge, with another around the corner.\n\nOur ability to deliver for our clients has never been stronger, with an enviable collection of global subject matter experts having joined the organization and new investments in internal training and development bearing fruit, we’re successfully delivering on some of the largest and most complex KM transformations, learning and content development initiatives, and knowledge graph and enterprise AI programs in the world. As other organizations are scrambling to jump on the artificial intelligence bandwagon, we’ve literally written the book on the confluence of knowledge, data, information, and technology as the foundation for enterprise AI, relying on our years of successful projects in the space to deliver enterprise-level solutions for our clients.\n\nAs I’ve done each year since starting this annual entry, I’ll use EK’s six guiding principles to further discuss the year in review.\n\n\n\nPeople – Our number one asset is our people. We invest in them and ensure they possess the knowledge and resources to serve our clients to the highest degree possible.\n\n\n\nThere is a very clear and specific reason that our first guiding principle is People. We are where we are because of our team, their expertise, their unique personalities, and the relationships they’ve forged with each other and our clients. Of course we have a slew of methodologies, benchmarks, and other proprietary materials we rely on to deliver, but at the end of the day (and the year, in this case), we rise because of our team. Ensuring the team is set up for success, feels valued, and chooses to grow with EK is my most important responsibility as CEO.\n\nIn years past, I’ve talked about many of the elements we’ve put in place to support our team. We’ve maintained and expanded all of the unique elements I’ve discussed in the past. Our continued learning benefit of $3,000 per year per employee was used at a higher rate than ever before, we added new modules and metrics to our year-long onboarding program called Kamp EK, and our Lifelong Learning benefit of $1,000 per year for non-work related development was used for some very cool new purposes including yoga, building a drone from scratch, landscape architecture, horseback riding, wine tasting, guitar lessons, and figure skating. The very best employees are those with a passion for learning, and EK’s support for that is an important and actionable way we demonstrate how we value our team and their unique interests.\n\nIn the office, we hosted yoga classes, massage days, pop-up juice parties, liquid nitrogen ice cream, wine tastings, and cocktail/mocktail making competitions. We also ran a few friendly competitions, ranging from a group-based steps challenge, a basil growing competition, a hydration challenge, and Know Shave Knowvember, all of which we translated to charitable giving from EK to a small collection of philanthropic partners. In total, we made nearly $30,000 of donations, yielding a real impact for our chosen organizations.\n\nOne notable addition to our cadre of programs this year was the creation of our Learning Cohorts. We recognized that our growth, coupled with the very unique expertise we possess, created a wonderful opportunity to further invest in the skills development of our team members. We already run a myriad of structured, more formal learning around a broad array of soft and hard skills, but we chose to supplement that with a less structured, more social learning opportunity. I first ran an eight-month long Learning Cohort on all things KM Strategy, Design, and Implementation. Meeting weekly, we actually used Making Knowledge Management Clickable as the loose teaching guide, covering a chapter per week, which I think was made more palatable when supplemented by the bottle of a good Napa Cabernet Sauvignon I brought each week. Leading that cohort was one of my favorite recent experiences, and it inspired additional Learning Cohorts led by others at EK on the topics of Project Management, Solution Architecture, and Ontology Design. The informal style of these sessions is a great match for our culture, and they’re a great way we’re modeling good KM behaviors and techniques of knowledge transfer and sharing as well.\n\nAs a result of these programs and our strong foundation and culture, EK was recognized by Inc. Magazine as one of the best workplaces in the United States for a fifth time! Below, I’ll share more about our national and global recognition, new wins, and other successes, but this win is always particularly meaningful to me as it is based on an anonymous survey of our employees, mirroring that the hard work we all put into making EK a great place to work, and for our people to thrive, continues to pay dividends.\n\n\n\nThought Leadership – We serve as leaders in the industry, sharing our knowledge and expertise, guiding the development of Agile knowledge and information practices, and supporting the community.\n\n\n\nWe hit a major milestone with our knowledge base this year, publishing our 500th article over the summer. The knowledge base includes a wide range of blogs, white papers, case studies, slide presentations, videos, and podcast episodes, all of which are free and open on our website. Collectively, they represent one of the deepest repositories of knowledge in our field. Our podcast, Knowledge Cast, was named the number one KM podcast for the third year in a row. At KMWorld, we tied last year’s record, delivering twelve separate presentations, including case studies alongside our clients at ASML, Ulteig, and Intel. On top of our longstanding presence at KMWorld, over twenty EK employees spoke at over a dozen different conferences across the fields of KM, Learning, Content, and Data this year. That was yet another record for us and a particularly notable one as it demonstrates the breadth of our talent.\n\nOur knowledge base, conference speaking, podcast, book, and bevy of experts all helped to secure new recognition from the industry. Again this year, KMWorld and Info Today recognized EK as one of the 100 Companies That Matter in KM for the ninth year in a row, as well as one of the AI 100: The Companies Empowering Intelligent Knowledge Management for the fourth year in a row.\n\nI’ve said for many years, I firmly believe that we are developing the next generation of leaders in the field(s). Though I always hope our people will choose to stay at EK forever, I’ve also been really proud to see that those who do choose to leave are consistently being hired for their next dream career positions, often going to very senior government posts, in-house to lead or build a department, or even to start their own company. It is always a bit melancholy to see an EK’er transition to alumni status, but seeing the amazing next opportunities we’re generating softens that blow. Our success in attracting new talent at all levels, including new established thought leaders and published authors, has also been an exciting counterbalance you’ll hear more about in the year to come.\n\n\n\nTransparency – We communicate clearly and openly, ensuring the highest level of quality and accountability for our company’s management, in our service to our clients, and with respect to our colleagues.\n\n\n\nOur commitment has always been to run EK as an open book, sharing company goals and performance with all employees at every level. That means sometimes we’re delivering bad news when we miss a target or hit a snag, but in those cases we’re doing it together and also collectively working to address the challenges and learn from them.\n\nThis past year we commenced a multi-stage reorganization to better accommodate our growth and successfully serve our clients. We call this EK 2.0, clearly a misnomer as we’ve gone through a myriad of changes in our nearly eleven years in business, but it felt right, given some of the major efforts and enhancements we planned and executed (and EK 14.0 just doesn’t have the same ring). One of my favorite days of the year was convening the entire company to discuss the reasons for the reorganization, explain all the moves, discuss the value of each component, and address any concerns collectively. We haven’t completed EK 2.0 by a fair stretch, but I am really proud of how the organization has helped us change and grow together as part of this.\n\nAs we’ve successfully moved to a remote-hybrid setup post-Covid, we’ve also updated our standard knowledge sharing and communications tools and practices. We’ve continued our bi-monthly all-hands employee knowledge share, where we discuss company goals and achievements, new wins, greet all new hires, and have employees present on new learnings or innovations, and added smaller breakouts and coaching sessions to check in on projects, staffing, and leadership, ensuring we’re arming our employees with the ability to lead and have a strong voice in the organization. Along with the cohorts, the one-on-one coaching I’ve picked up with some team members has become some of my favorite hours of each week.\n\nWe’ve also built upon our in-person traditions, coming together at key moments of the year for our Annual Winter Gala, Summer Pirate Ship Cruise, and Holiday Purple Elephant. This last year the Gala was particularly special, as we rang in June 11, 2023, the official 10-year anniversary of EK’s foundation at the Renwick Gallery of the Smithsonian American Art Museum, decked out in our finest. We preceded these in-person ceremonies with in-person strategic planning sessions, live training, and other opportunities to celebrate our collective successes.\n\n\n\nPartnership – We partner with our clients, building meaningful relationships founded on a sustained commitment to mutual success.\n\nThe word “Partnership” was chosen very carefully when we first crafted our guiding principles in 2013. It expresses our vision to be true consultative partners and trusted advisors to our clients, rather than generic order takers. Though we’ve grown to deliver enterprise-level initiatives and offer the complete lifecycle of knowledge, information, and data management services from strategy, to design, through implementation and operations, that commitment to real partnership has remained a constant for us.\n\nMany of you might have seen the messaging we put out in the fall to signal a major recruiting effort. The need for this recruiting push was driven by our successful partnerships, with EK’s marquis clients choosing to not just continue their work with us into 2024, but selecting us as their chosen trusted partner, asking us to do more with them, and committing to larger and more impactful engagements that reach the highest levels of the organizations. Moving beyond the financials, 2023 was another year where many of our clients chose to co-present with us at conferences, and still others, like the European Central Bank and Green Climate Fund chose to recognize our work online and unprompted, again demonstrating that they view us as true and longstanding partners in the same way we do towards them.\n\nOur successes in building these long-term partnerships mean that we’re able to support the professional community in new and exciting ways. This past year we joined the EDM Council, a global data management association, in order to exchange and contribute to trusted and shared data management frameworks and industry standards to shape the future of data and analytics. In addition, we sponsored a number of conferences including KMWorld and the Midwest KM Symposium. In 2024, we’re sponsoring the launch of a new conference in Europe, Knowledge Summit Dublin , which I’m particularly excited to participate in and speak at.\n\nThese successes also mean an impact on the greater community. I already detailed our philanthropic contributions this year, but I wanted to note in particular the fact that we crossed the threshold of $100,000 contributed over EK’s history to the Wolf Trap Institute for Early Learning Through the Arts . It is exciting to think that because of the work we do with our great clients, a lot of kids in this world have gained access to arts and music education they otherwise would have lacked.\n\n\n\nIntegration – We provide our customers with the full range of EK’s expertise, integrating all of our services and resources to ensure the most significant business value.\n\nBack in 2013, we chose the concept of Integration largely to express the coming together of our different areas of expertise. Over the years, we’ve assembled an incredible team of experts with backgrounds in Knowledge Management Strategy, Knowledge Transfer, Data Management and Governance, Information Management, Content Strategy and Assemble, Learning, Taxonomy and Ontology, Knowledge Graphs…the list goes on and on. This alone is unique, but more than just bringing these experts together under a single roof, we’ve fostered a community of collaboration and kindness where they’re working together, learning from each other, and blazing new trails in their individual fields based on the ideas and learnings from their colleagues.\n\nAs EK has grown, and of particular note in 2013, an additional meaning of integration has come into focus for us. When I consider the common thread between all of our services, it is this idea of integrating information to eliminate silos across an organization. All of our services, all of our people, and all of our projects are, at their core, about integration and connections. We’re using semantic layers to integrate different types of data, information, and knowledge across the enterprise, adding context, and ensuring that collective material is more actionable and easier to find. We’re connecting people and the knowledge they hold so that it may be retained by the organization and leveraged more effectively for collaboration, innovation, and learning. Increasingly, we’re integrating all of an organization’s knowledge assets and enhancing them with context and meaning in order to drive an organization’s AI initiatives. This is what we’ve always done, it’s always been a part of our mission, but that common thread is clearer than it’s ever been.\n\nTo feed this continued integration of our services and leadership in the combined fields driving AI, we organized all of our Knowledge, Information, and Data Management services under a single sector. We’ve always held that considering all of an organization’s knowledge assets collectively hold the greatest value for the organization, but increasingly, that is also translating to delivering solutions that bridge these gaps. Though the move would’ve been met with skepticism even five years ago, it is a natural progression of how we’ve been operating and expresses the manner in which we’ve always approached Knowledge Management: as the complete continuum of knowledge assets.\n\n\n\nEnergy – We share our enthusiasm with our clients and colleagues, leveraging our excitement to achieve meaningful change.\n\n\n\nIn the midst of a burst of growth, new clients, and new team members, and having just closed out our tenth year of sustained growth, I don’t think I’ve ever been more enthusiastic about the year in front of me. The energy we bring to our clients and colleagues is what drives me to do more at EK and ask, “What’s next?” with excitement and enthusiasm.\n\nOn behalf of Enterprise Knowledge and the EK Group, I thank you for your partnership and wish you a wonderful 2024!"
    },
    {
        "title": "3 Steps for Building the Right KM Team",
        "url": "https://enterprise-knowledge.com/3-steps-for-building-the-right-km-team/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "km",
            "km team framework",
            "km vision",
            "skills",
            "team"
        ],
        "article_type": "blog",
        "content": "At Enterprise Knowledge (EK), a lot of our knowledge management (KM) work in support of our clients surfaces the need for, and questions around, having a KM team in place. The right KM team – usually 2-5 core individuals who oversee KM initiatives and divide KM-related responsibilities between them – will combine the right skill sets, like collaboration and strategic planning expertise, to systematically design and implement KM at your organization. According to Harvard Business Review , “companies in the Fortune 500 still lose a combined $31.5 billion per year from employees failing to share knowledge effectively,” and we at EK have observed that an ineffective KM team is one of the top reasons why KM fails, as it is unable to give KM the focus it needs to embed it at the organizational level, garner buy-in from stakeholders, align with strategic goals, and lay the foundation for continued sustainment and KM evolution.\n\nBut how do you go about actually building that team? Whether KM at your organization consists of just you, you and a small team, or an entire division, here are three simple steps for cultivating a successful KM team that provides the greatest value to you and your organization.\n\n\n\n1. Understand Your Organization\n\nBefore you even begin your search for a new KM team member, it’s important to take the time to first define your organization’s specific needs:\n\nIf you don’t have one already, develop a consistent, distinct KM vision based on your responses to the above questions. Keep your wording simple, prioritizing substance over style. This will help you build momentum for KM and provide an overall indicator of success, as well as support change management efforts later on as you implement KM initiatives with your team. Need inspiration? Try emulating the sample KM vision statement for a large bank below.\n\n—\n\nBank staff are able to easily find information and get trained on streamlined HR processes – improving HR’s ability to monitor compliance with Bank’s ethics framework and increasing cost efficiencies – because Bank’s systems are more integrated and accessible via an enterprise search capability.\n\n—\n\nOnce you’ve arrived at a clear organizational definition and purpose for KM, outline the basic skills and capabilities that would demonstrate a proficiency in these concepts. If possible, build on any KM job descriptions that already exist to ensure you’re capturing all available resources. As you begin to list out the qualities and skills a KM team member should possess, categorizing your criteria will allow you to mitigate any overlap or duplication of competencies as you brainstorm. We recommend incorporating the following key competencies to be successful in KM:\n\nThis competency list will not only support you as you add new members to your KM team, but also provide you with a useful resource for conducting skill gap analyses with existing members to build their strengths, apply targeted trainings to their weaknesses, and better equip them to succeed in their KM roles.\n\n\n\n2. Customize Your Team Framework\n\nOnce you’ve consolidated your KM definition and brainstormed a list of KM-related skills and characteristics, you can leverage them to develop your KM role(s) and job description(s). A good KM team structure isn’t too rigid, and allows for continual evolution and flexibility depending on organizational change. It is important to start small, focusing on building a strong team one person at a time, and a good foundational structure makes all the difference in maintaining a collaborative and trusting culture as the team grows.\n\nOur approach to defining a KM team structure revolves around determining the team’s day-to-day duties and the roles you’ll need to execute them. We recommend you consider these in the context of a corresponding decision-making and governance model that accounts for meeting cadence, collaboration tools, goal setting, and success criteria. For instance, you might want a Collaboration Lead to design and implement collaboration activities and a Search & Technology Lead to capture requirements for tool selection. You’ll also need a KM Sponsor to be your champion on the executive leadership team who can advocate for your KM budget and prioritizing KM initiatives (for a rumination on where a KM organization should sit within an organization, check out our CEO Zach Wahl’s blog ).\n\n\n\n\n\n\n\nPrioritize the most critical position(s) for your organization in its current state. Roles should closely align with the KM challenge(s) you identified during visioning: which type of role(s) will best enable you to address them? And how will these roles interact with each other? See an example of a decentralized model above.\n\n\n\n3. Start Building Your KM Team\n\nNow that you’ve determined your KM role(s), the next step is to determine if you have the right skill sets and people in-house or if you need to hire externally. Identify any critical skills considered essential to working at your organization. For instance, if you operate in a niche sector of an industry (e.g., renewable energy engineering), you may need to include a corresponding educational requirement in your job description and evaluation criteria. Begin to map these essential responsibilities to your role(s) and KM competencies, thinking critically about how each role will interact with others and the processes that might already be in place.\n\nBuilding and sustaining a culture that champions knowledge management and sharing involves bringing in people who share your organization’s values. In selecting from a pool of external candidates, many organizations place more significance on an individual’s abilities (e.g., strategic thinking) and skills (e.g., software proficiency) than on their values and whether they will be a good “culture fit;” ensure you have a healthy mixture of hard and soft skills to maintain that balance. While an external hire may be a gamble, they often come with valuable outside experience and might just bring the fresh perspective you need to take the next step with KM.\n\nHiring internally , however, can look a little bit different. When evaluating internal candidates, you will likely have a number of supplemental resources at your disposal, including past work, performance reviews, and trusted individuals who can vouch for them. This knowledge of their cultural synergy with your organization will prove indispensable in evaluating their cultural fit, but be careful to take an objective view – having all of this background information may factor into bias.\n\nReview your list and determine which skills and capabilities are required as a must-have, and then which characteristics would be nice to have. These preferred, non-critical abilities should be supplemental and teachable, making it easier to upskill candidates once they’ve been brought onto the team; some candidates will be more specialized in their KM experience and expertise than others, so a holistic view will help you achieve the best result.\n\nIn the end, you should have (1) job description and (2) lists of itemized criteria for the best-fitting KM team members for your organization. For example, the team skills your candidates should possess might look something like the image below.\n\n\n\n\n\nWith a customized framework, refined job descriptions, and criteria tailored to each role in hand, you can confidently begin the process of evaluating candidates and filling the positions you’ve deemed most essential to your organization in its current state, ultimately building a strong foundation for KM initiatives that promotes team collaboration, knowledge sharing, and professional growth.\n\n\n\nConclusion\n\nNow that you have the right team on board, you can begin to socialize KM best practices to the rest of your organization with a renewed vigor and targeted customization. EK is prepared to support you on the next phase of your journey: garnering support and momentum for your KM initiatives to sustain them long-term. For help making KM stick at your organization, check out our blogs on leading change and developing good KM habits , or contact us here ."
    },
    {
        "title": "Taxonomy Roller Coasters: Techniques to Keep Stakeholders on the Ride",
        "url": "https://enterprise-knowledge.com/taxonomy-roller-coasters-techniques-to-keep-stakeholders-on-the-ride/",
        "categories": [
            "change management and communications",
            "taxonomy ontology design"
        ],
        "tags": [
            "change management",
            "stakeholder management",
            "taxonomy"
        ],
        "article_type": "presentation",
        "content": "Laurie Gray, Principal Consultant on Enterprise Knowledge’s Strategy team, and EK client Kate Vilches, Knowledge Management Lead at Ulteig, presented on November 6, 2022 at the Taxonomy Boot Camp Conference, co-located with KMWorld, in Washington, D.C. The talk, “Taxonomy Roller Coasters: Techniques to Keep Stakeholders on the Ride,” focused on proven stakeholder management techniques during enterprise taxonomy development and launch activities.\n\nGray and Vilches used their firsthand experience to relate advice, share practical tools, and provide real-life examples to ensure successful stakeholder involvement, reinforcing three key themes for attendees:\n\nAre you ready to begin taxonomy efforts at your organization, or are you in the middle of a taxonomy effort that you need assistance with? Contact Enterprise Knowledge today!\n\nSlide Deck"
    },
    {
        "title": "Case Studies: Applications of Data Governance in the Enterprise",
        "url": "https://enterprise-knowledge.com/case-studies-applications-of-data-governance-in-the-enterprise/",
        "categories": [
            "knowledge graphs data modeling"
        ],
        "tags": [
            "benchmark",
            "data catalog",
            "data governance",
            "data maturity"
        ],
        "article_type": "presentation",
        "content": "Thomas Mitrevski, Senior Data Management and Governance Consultant and Lulit Tesfaye, Partner and Vice President of Knowledge and Data Services presented “Case Studies: Applications of Data Governance in the Enterprise” on December 6th, 2023 at DGIQ in Washington D.C.\n\nIn this presentation, Thomas and Lulit detailed their experiences developing strategies for multiple enterprise-scale data initiatives and provided an understanding of common data governance and maturity needs. Thomas and Lulit based their talk on real-world examples and case studies and provided the audience with examples of achieving buy-in to invest in governance tools and processes, as well as the expected return on investment (ROI).\n\nCheck out the presentation below to learn:\n\nSlide Deck"
    },
    {
        "title": "Knowledge Cast – Barbara Fillip of Chemonics International",
        "url": "https://enterprise-knowledge.com/knowledge-cast-barbara-fillip-of-chemonics-international/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "ai",
            "knowledge management"
        ],
        "article_type": "podcast",
        "content": "Enterprise Knowledge CEO Zach Wahl speaks with Barbara Fillip, Senior Advisor of Knowledge Management at Chemonics International and adjunct professor at George Mason University. Founded in 1975, Chemonics is a leading global sustainable development firm with 6,000 experts in more than a hundred countries around the globe.\n\n\n\nIn their conversation, Barbara emphasizes the importance of understanding our own knowledge habits, the exciting possibilities that knowledge management presents as a bridge to artificial intelligence, and the challenge of moving valuable knowledge from community discussions and social networks into a prominent and lasting place within an organization.\n\n﻿ ﻿ ﻿ ﻿\n\n\n\n\n\n\n\n\n\nIf you would like to be a guest on Knowledge Cast, contact Enterprise Knowledge for more information."
    },
    {
        "title": "Expert Analysis: Reusable Learning Content",
        "url": "https://enterprise-knowledge.com/expert-analysis-reusable-learning-content/",
        "categories": [
            "advanced content",
            "enterprise learning"
        ],
        "tags": [
            "advanced content",
            "enterprise learning"
        ],
        "article_type": "blog",
        "content": "Most everyone in their career has had to sit through a training that was too general, too long, or too outdated. In this day and age people expect a more tailored experience and more flexibility in how they can learn…but how do you do that?\n\nIn this blog, two of our expert consultants, Emily Crockett and Deneena Lanius , will walk you through what reusable learning content is, the basics of how to implement it, and the benefits your organization can expect once it has been implemented.\n\nQuestion 1: What is reusable learning content and how is it different from what organizations do today?\n\nEmily Crockett\n\nReusable learning content is NOT a copy. Reusable learning content is the same object used in multiple contexts across learning materials, courses, or programs. While copies of documents worked for Learning and Development (L&D) professionals when that was the only option for creating differentiated and personalized learning, those copies have created a learning content explosion and version control nightmare. This means that content management becomes really difficult! Content can start to get lost because there is so much to look through, people get frustrated so then they just recreate it, more copies are created, so and so forth.\n\nPerhaps your organization created a compliance training course that uses 85% of the same content across departments while the remaining 15% is department-specific. In the “copy world,” each department would have separate copies of the training to keep updated. When changes need to be made to the 85% of the content that should be the same for everyone, someone has to change it in every copy created to maintain consistency across the organization. Content contributors would also need to know where all copies are stored to accomplish updating the courses. In the “content reuse world,” the 85% of content shared across the courses all comes from the same source objects and you can see clearly where those distributed links are. When you have to update the 85% shared content, you only need to update it once to update every version instance.\n\nDeneena Lanius\n\nLearning and development professionals have always provided differentiated instruction, but the old approach of making copies of the original learning asset and personalizing it for the needs of our particular learners creates an unsustainable content maintenance burden. Reusable learning content allows us to achieve differentiated instructional materials in a more extensible way.  Reusable learning content streamlines efforts by repurposing materials, saving time, maintaining consistency, and enriching the overall learning experience.\n\nEnterprise Learning focuses on using reusable learning content that allows us to personalize learning experiences efficiently. The following scenarios illustrate a few examples of how this personalization happens:\n\nReusable learning content redefines content efficiency by repurposing materials across diverse scenarios, setting this approach apart from the conventional practice of crafting separate resources for each training, thus elevating learning outcomes through streamlined efforts and enriched experiences.\n\nQuestion 2: What technology stack is necessary for creating and managing reusable learning content?\n\nDeneena Lanius\n\nIn the realm of learning and performance, content diversity extends beyond traditional classroom materials. Content diversity is a blend of documents, videos, and interactive modules from experts and experienced contributors. Involving all creators, not just trainers, is a game-changer, adding unique performance resources for learners. A typical Learning Management System (LMS) cannot handle this content diversity. LMSs often function as standalone repositories for structured course delivery. The system encounters difficulties handling reusable and dynamic content due to its singular repository design. This limitation poses challenges in accommodating the dynamic adaptability and repurposing needed for diverse content across various contexts.\n\nEmily Crockett\n\nTo optimally implement reusable learning content in your organization, a Content Management System (CMS) or LMS system that only stores documents or pages won’t cut it. You need to be able to store smaller learning objects or components for distribution across a holistic learning ecosystem. A learning ecosystem consists of an environment where learners interact with content in several ways, and their interactions are then shaping how content changes to meet their needs. The graphic below demonstrates the ideal content operations systems:\n\nThis learning technology stack connects to delivery channels such as Learning and Performance portals , LMSs or Learning Experience Platforms (LXPs), Mobile Apps, or any number of content delivery channels. An API (or integration) layer orchestrates this connection and enables different systems and channels to “talk” to each other and exchange content and data. The final piece in a learning technology stack is a Learning Record Store (LRS) that can collect data about your learner’s learning experience using the Experience API or xAPI standard. A LRS provides part of the feedback necessary to drive relevant changes to content so that learning content can better meet the needs of your learners. This learning ecosystem allows you to build more impactful reusable learning content and manage that content in an agile and efficient manner.\n\nQuestion 3: What processes and roles are necessary for creating and managing reusable learning content?\n\nDeneena Lanius\n\nInvesting in learning ecosystems and cultivating diverse roles within L&D teams is imperative for creating and managing reusable content. Organizations should recognize the essential nature of these investments to establish collaborative, cross-functional, and efficient teams dedicated to Learning and Development (L&D). Instructional designers collaborate with subject matter experts to design content that aligns with learning objectives. If we add a content engineer to this design team, we add the capability to design learning content in task or learning objective sized components suitable for reuse. Meanwhile, content owners, content developers and multimedia specialists step in during content creation, teaming up to fashion an array of learning assets encompassing documents, interactive modules, and multimedia components. This collective orchestration interfaces with the established technology stack, empowering these roles with the tools for efficient content reuse.\n\nThe subsequent stages revolve around maintaining content quality and organization. Content reviewers ensure accuracy, while quality assurance professionals uphold design standards. Learning curators and knowledge managers tag and organize content for easy retrieval and categorization. Moreover, learning experience designers take on the task of repurposing existing content to suit different learning contexts. Lastly, the measurement and feedback aspect involves learning analysts monitoring content usage and learner feedback to drive continuous enhancements. Collaborative cross-functional teams, consisting of content contributors and experts, bring diverse perspectives to the content creation process.\n\nEmily Crockett\n\nManaging reusable learning content is not unlike managing Advanced content with roles to address content strategy, content operations, content design, and content engineering. The key differentiator to create and manage reusable learning content is the content engineer working in tandem with the instructional designer . The instructional designer defines what should be reused based on best practices and content strategy, and the content engineer defines how content reuse takes place across content types and outputs based on a content model.\n\nAnother key role in enabling reusable content is a taxonomist . To leverage reusable content, you need to be able to find the components, meaning there needs to be a standard language to enable that type of search. Furthermore, to develop different types of adaptive learning or recommender engines for learning content , there needs to be a foundational taxonomy that drives an ontology, which in turn supports Machine Learning (ML) and Artificial Intelligence (AI) . Additionally, with a more technically complex learning ecosystem, it is important to have a designated role for managing the technical complexities such as the integrations and APIs of a learning ecosystem.\n\nGovernance processes are critical to consider early in your reusable learning content journey. Reusable content is not a “set it and forget it” endeavor, and establishing good scalable governance procedural processes will preserve learning content functionality and reusability for years to come. When enforcing learning content governance, it is important to establish both change management processes and training since creating reusable learning content is fundamentally different from how content was created and maintained before. Establishing a standard process to train new and old instructional designers will help encourage adoption and best practices in maintaining content.\n\nQuestion 4: What are the benefits of reusable learning content to organizations and learners?\n\nDeneena Lanius\n\nWhen designing a learning environment , reusable learning content unlocks a host of advantages for both L&D teams and learners, and is a transformational approach in the realm of education and training. The following are three benefits that revolutionize the learning landscape through reusable content:\n\nEfficiency in Action: Recognizing the potential, reusable content becomes a time-saving powerhouse by streamlining content creation for L&D teams. Reusable learning content allows instructional designers and content creators to fine-tune and enhance existing resources rather than starting from scratch.\n\nConsistency Empowers : Imagine a symphony’s harmonious notes coming together. Similarly, consistently reusing content creates a harmonized learning experience. Reusing content is not about repetition, but about reinforcing key concepts across different modules, fostering clarity and a unified learning journey.\n\nAdaptability Unleashed : Learners benefit from content tailored to their personas, contexts, and proficiency levels. It’s like having a personalized guide through the learning landscape, ensuring engagement and deep comprehension.\n\nThe adoption of reusable content heralds a new era in education and training, promising enhanced efficiency, consistency, and adaptability for educators and learners.\n\nEmily Crockett\n\nKey benefits to an organization include:\n\nDecreased cost in creating and maintaining content: Imagine if every time a car company designed a new car, they started over from scratch. That car design process would get expensive! So why are organizations starting from scratch when they design learning engagements? If there is standard or typical content to include in every engagement, or standardized content to build off of, and instructional designers don’t have a standardized library to leverage for reuse, then you’re essentially paying them to do something they’ve already done and developed. Additionally, maintaining numerous copies from the perspective of data storage and the design time it takes to revise each copy, is incredibly expensive.\n\nDecreased time responding to compliance issues: Any endeavor comes with risks, and hopefully your organization has safeguards in place to mitigate these risks. However, how does your organization handle a situation where you must pull content from a learner’s exposure? For example, if there is a rights issue with an image used in a couple of courses, what is the protocol to remove that image? Would a frantic search for every place this content appears take place? By leveraging reusable content, you can more easily find all those places, and replace them at the source to propagate out. This decreases the time it takes to correct the mistake and the likelihood that a copy of that content still exists, thereby protecting your organization from additional risk.\n\nIncreased brand consistency: Just like a design system allows your organization to create reusable design components to encourage cohesion, using reusable learning content allows your organization to establish a foundation of brand-compliant content. This branding cohesion supports your image as an organization increasing the trust your customers have in your brand. Furthermore, if your brand changes and you’ve used reusable content, you can quickly change logos and other key brand elements from a single source, rather than dealing with trying to change all of the copies and inevitably missing something.\n\nConclusion\n\nThe incorporation of reusable learning content represents a transformative approach in education and training, streamlining efforts and personalizing learning experiences within Enterprise Learning environments. This shift challenges outdated practices, highlighting the inefficiency of creating separate resources for each training, and emphasizes the need for a strategic and sustainable approach in content creation, reusability, and management. Investing in learning ecosystems and cultivating diverse roles within L&D teams is paramount for efficient content operations. Through collaborative efforts during content analysis and design, instructional designers align content with learning objectives, while content owners, developers, and multimedia specialists contribute to a diverse array of learning assets. This orchestrated collaboration, supported by an effective technology stack, empowers organizations to fully leverage the advantages of reusable learning content, reducing costs, enhancing compliance responsiveness, and fostering increased brand consistency. If you’re interested in creating reusable learning content in your organization, contact us today!"
    },
    {
        "title": "Breaking it down: What is a Knowledge Portal?",
        "url": "https://enterprise-knowledge.com/breaking-it-down-what-is-a-knowledge-portal/",
        "categories": [
            "company",
            "content brand strategy",
            "strategy design",
            "software development"
        ],
        "tags": [
            "knowledge portals"
        ],
        "article_type": "blog",
        "content": "Throughout my career, I have worked with dozens of public and private organizations who want a holistic understanding of their knowledge assets. Organizations seek new ways to manage, find, and take action on their knowledge, whether through a content management system, search tool, or GPT chatbot.\n\nAt EK, we’ve identified a common challenge faced by many of our clients: the presence of siloed data sources and dispersed information, which both internal and external users require access to on a daily basis. This observation led us to the idea of Knowledge Portals.\n\nKnowledge Portals are a compelling solution for organizational findability and discoverability.\n\nWhat is a Knowledge Portal?\n\nEK defines a Knowledge Portal as\n\nthe hub to integrate your organization’s KM assets, including your information, data, and people, into a single highly contextualized environment where real business can get done.\n\nThis definition touches on the three main differentiators between knowledge portals of the past and today. Knowledge Portals:\n\nLet’s look at some examples as we break down these three differentiators.\n\nKnowledge Portals Contain All Your Stuff\n\nThe Knowledge Portal solution combines an organization’s knowledge, content, data, and people into a single view. Previously, Knowledge Portals surfaced content across repositories and provided links to each system. Now, Knowledge Portals contain experts, company events, projects, previously hosted training videos and webinars, and essential policies. On the Knowledge Cast podcast on Knowledge Portals , my colleague Rebecca Wyatt stated,\n\n“…users need one place to [search] and get the information that they need. You’ve already failed if they have to decide where to go to execute a search…”\n\nToday’s organizations have a proliferation of systems and information as teams look to solve their own specific use cases. A Knowledge Portal breaks down those information silos and surfaces relevant information to users at query time. Additionally, Knowledge Portals provide the ability to intuitively navigate an organization’s domain. For example, from viewing a project, one can see related experts on that project and then move on to relevant training videos. Curating all of your knowledge assets in one portal enables more intuitive user journeys and more efficient work as users take action on the knowledge they find.\n\nKnowledge Portals Contextualize Information\n\nKnowledge Portals are designed to consolidate the most comprehensive information about a specific “thing” within your organization into a single, accessible location. Sometimes referred to as an entity , a “thing” is a key element in your organization’s domain , such as people, products, communities of practices, or subject areas. These entities are often so ingrained in an organization’s work that they appear across multiple systems. A Knowledge Portal effectively integrates this separate information into a cohesive view, which provides context to users, enabling them to see how these entities interconnect with and contribute to the broader scope of an organization’s work.\n\nAs we introduce Knowledge Portals to clients, a common question arises regarding how they differ from Enterprise Search. Indeed, there are notable similarities between the two. Both Knowledge Portals and Enterprise Search:\n\nHowever, despite these similarities, the unique functionalities of Knowledge Portals set them apart in significant ways, which we’ll explore in further detail.\n\nEnterprise Search is a major component of a Knowledge Portal. The core differentiator of a Knowledge Portal is its ability to establish relationships between information across various systems and to identify the elements that comprise an entity. Consider this scenario: when you search for a company project using an Enterprise Search tool, you receive a list of individual items (such as people, documents, and projects) related to or mentioning that project. In contrast, a Knowledge Portal takes you directly to a comprehensive project page. This page not only describes the project but also details the team members, client contacts, project deliverables, and offers direct links to the profiles of each person, client, or other relevant entities associated with that project. That’s the key feature. Knowledge Portals connect your organization’s entities so that users can get away from a simple set of results and intuitively access and understand the information they need.\n\nKnowledge Portals Personalize and Enable\n\nIn the process of assembling and contextualizing information, knowledge portals tailor the content and information displayed for each individual user. A crucial part of a knowledge portal is the underlying graph that maps out the relationships between the organization’s entities. This graph, based on shared characteristics and previously obscured connections, enables us to connect users to other organizational entities in that graph. For example, if the graph includes a user’s job role, the portal can then tailor the display of upcoming events or courses on the homepage or within search pages to suit that role. Similarly, if the graph includes information about a user’s department, we can boost search results related to projects or deliverables from the same department.\n\nAdditionally, by considering a user’s role, we can provide targeted actions along with the information provided. For instance, project managers viewing employee profiles could benefit from an “add to team” button when identifying subject matter experts. Similarly, customers browsing a product-focused knowledge portal could have access to a “talk to an agent” option, whereas support agents would see solution guides to assist them in walking customers through various issues. This level of personalization, combined with action-oriented opportunities empowers users to take action quickly and successfully.\n\nWe encourage scheduling a few workshops focused on action-oriented search and knowledge graph design to better understand what users would find most beneficial as they interact with the Knowledge Portal. The potential for personalization, along with the ability to deliver targeted messages and actions are endless.\n\nConclusion\n\nKnowledge Portals have become increasingly critical for organizations in the face of growing user information demands and the rapid expansion of knowledge. If you’re interested in learning more about Knowledge Portals and their successful implementation, I recommend some insightful blogs and case studies authored by my colleagues:\n\nIf you want to collaborate on the development of your organization’s Knowledge Portal, or if there’s a specific topic you’d like us to explore in future discussions, don’t hesitate to contact us !"
    },
    {
        "title": "Scaling Knowledge Graph Architectures with AI",
        "url": "https://enterprise-knowledge.com/scaling-knowledge-graph-architectures-with-ai/",
        "categories": [
            "ai",
            "knowledge graphs data modeling"
        ],
        "tags": [
            "ai",
            "extraction",
            "knowledge graphs",
            "scaling"
        ],
        "article_type": "presentation",
        "content": "Sara Nash and Urmi Majumder, Principal Consultants at Enterprise Knowledge, presented “Scaling Knowledge Graph Architectures with AI” on November 9th, 2023 at KM World in Washington D.C. In this presentation, Sara and Urmi defined a Knowledge Graph architecture and reviewed how AI can support the creation and growth of Knowledge Graphs. Drawing from their experience in designing enterprise Knowledge Graphs based on knowledge embedded in unstructured content, Sara and Urmi defined approaches for entity and relationship extraction depending on Enterprise AI maturity and highlighted other key considerations to incorporate AI capabilities into the development of a Knowledge Graph. Check out the presentation below to learn how to:\n\nSlide Deck"
    },
    {
        "title": "Learning Strategy Assessment & POC",
        "url": "https://enterprise-knowledge.com/learning-strategy-assessment-poc/",
        "categories": [
            "content brand strategy",
            "enterprise learning"
        ],
        "tags": [
            "enterprise learning",
            "knowledge management",
            "learning strategy",
            "poc"
        ],
        "article_type": "product",
        "content": "Our Learning Strategy Proof of Concept (POC) aligns your learning strategy with business goals and efficiently validates improvements with a clickable POC. Prove the value of your updated learning strategy, leverage observed outcomes, and take actionable steps to transform your learning ecosystem for maximum business impact.\n\nWhy Learning Strategy Assessment & POC\n\nWe ensure that your investment in learning and development yields substantial returns, empowering your workforce and driving business growth. We accomplish this by:\n\nDownload the EK Learning Strategy Assessment\n\nOutcomes\n\nEK’s Learning Strategy Assessment & POC can help your organization:\n\nThere are three available sizes of Learning Strategy Assessment & POC engagements that are built to best fit your organization’s budget and needs, each of which can be viewed with more detail at the link. Are you looking to upgrade and modernize your learning strategy? Contact us here ."
    },
    {
        "title": "Knowledge Cast Product Spotlight – Matthieu Jonglez of Progress",
        "url": "https://enterprise-knowledge.com/knowledge-cast-product-spotlight-matthieu-jonglez-of-progress/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "ai",
            "knowledge management"
        ],
        "article_type": "podcast",
        "content": "Enterprise Knowledge COO Joe Hilger speaks with Matthieu Jonglez, Vice President of Technology at Progress. Progress’ mission is to be the trusted provider of the best products to develop, deploy, and manage high impact applications. Two of them, MarkLogic and Semaphore, are used to solve complex data problems by delivering data agility.\n\n\n\nIn this episode, Joe and Matthieu discuss how MarkLogic and Semaphore address customer needs, the importance of context with your data, and the future of Artificial Intelligence at Progress.\n\n﻿ ﻿ ﻿ ﻿\n\n\n\n\n\n\n\n\n\n\n\nIf you would like to be a guest on Knowledge Cast, contact Enterprise Knowledge for more information."
    },
    {
        "title": "4 Critical Elements of a Successful Data Governance Program",
        "url": "https://enterprise-knowledge.com/4-critical-elements-of-a-successful-data-governance-program/",
        "categories": [
            "knowledge graphs data modeling"
        ],
        "tags": [
            "data governance",
            "operating model",
            "processes",
            "roles responsibilities",
            "solutions architecture"
        ],
        "article_type": "infographic",
        "content": "Without a strong data governance framework, maintaining your organization’s data can become an unwieldy challenge: with unclean, decentralized data, staff may begin to lose trust and confidence in the information they are working with. If you’re unsure where to start, or what to focus on, we’ve outlined the four key elements required to facilitate enterprise-wide adoption of a data governance program at your organization.\n\nIf you are exploring ways your organization can benefit from implementing a data governance program, we can help! EK has deep experience in designing and implementing solutions that optimize the way you use your knowledge, data, and information, and we can produce actionable and personalized recommendations for you. Please contact us for more information.\n\nSpecial thank you to Nina Spoelker for her contributions to this infographic!"
    },
    {
        "title": "QA for Personalized Content",
        "url": "https://enterprise-knowledge.com/qa-for-personalized-content/",
        "categories": [
            "advanced content",
            "agile",
            "strategy design",
            "software development"
        ],
        "tags": [
            "agile",
            "content personalization",
            "quality assurance"
        ],
        "article_type": "blog",
        "content": "We’ve all sifted through dense technical documentation which has way too much detail about features and products that aren’t even relevant to us – just to get to that one nugget of information we’re actually looking for (often on page 245 of a 500-page product manual). If we deliver personalized content to our end users, we can solve this problem by only showing people the information that’s relevant to them.\n\nHowever, delivering personalized content introduces a specific set of Quality Assurance (QA) considerations. Quality Assurance (QA) is defined as the maintenance of a desired level of quality in a service or product, especially by means of attention to every stage of the process of delivery or production. It is vital to the success of application development and product delivery that QA and User Acceptance Testing (UAT) are carried out as an ongoing part of the release cadence of an application. QA and UAT help eliminate accrued technical debt by raising any and all issues throughout development while ensuring the application is being built with personalization in mind through the continuous feedback loop of QA.\n\nThat said, there should be a clear set of standards in a QA test plan to capture and evaluate how personalization is implemented throughout development. In this blog, I’ll highlight some key tenets for a development team to follow to ensure personalization is continuously captured and reinforced through the thoughtful use of QA.\n\nAgile Release Cadence and QA Scripting\n\nWhen developing an application, releases are structured to follow a set cadence as defined by a release period. Each release will contain the features, patches, and other deliverables to the application developed by the team’s engineers during that release period. It is important to ensure proper UAT is carried out during each release period and accepted before the application is released to production. This starts with defining a QA script for users to follow when executing UAT and exhaustively testing the code contributions and their associated use cases within the application. Moreover, when personalization comes into play, criteria must be included in that UAT script to not only ensure code contributions are accepted but to validate that what’s delivered achieves personalization.\n\nCriteria that evaluate personalization or personalized content delivery must be properly defined within these QA scripts and in a way that is easily executable by the users testing proposed changes to the application. These criteria should be translated from the business requirements and personalization goals the engagement tries to solve. For example, consider search engineering for a complex technical resource center for a large business with many departments. Without personalization, UAT could be passed by searching multiple facets and seeing results that are tagged or belong to said facets. With personalization in the QA script, UAT for those same search results is extended to cover things such as boosting results based on a user’s location and content findability based on a user’s role.\n\nA successful development team knows the business requirements, personalization efforts, and use cases implemented by ongoing development. As a result, they will have an increased understanding of how to structure QA scripts so that proper evaluation of personalization is fully and accurately defined. In this way, the team will be able to better provide users with a testing plan to uncover more possibilities of bugs in the application, shortcomings of the feature(s) developed, and overall integrity of the application’s end-to-end functionality; all the while ensuring personalization is measurable and fully understood by the entire team. Uncovering and understanding these shortcomings early can eliminate technical debt by reducing the chance of discovering them after the application has gone live.\n\nQA Processes and Execution\n\nAfter QA scripts have been properly defined and distributed to users, it is time to carry out QA and ensure UAT is successful for the application to move to production.\n\nAs stated above, the development team has a much deeper understanding of the features proposed in the release, which could negatively impact the integrity of QA execution. An engineer, for example, executing QA scripts could produce a false sense of a feature being intuitive, given their foundational understanding of how it works. Instead, execution should be left to the users as they will offer the most critical and valuable feedback. There should also be a well-defined place for users to indicate a pass or fail status, along with a dedicated space for feedback and replication steps of any issue.\n\nSuppose an engineer has developed a feature that maps content metadata to user roles after publishing the content. The code involved has been vigorously unit-tested and functionally tested by the engineer, giving them a biased lens of an intuitive feature because of their familiarity. A user will provide unbiased feedback on the feature, as they will be exposed to it with a fresh set of eyes. This will allow the user to evaluate the feature for accurate personalization, allowing for a continuous feedback loop of business goals being properly accomplished for the entirety of the QA process.\n\nMeaningful Results and Feedback\n\nUpon completion of QA and UAT, it is time to review the feedback that users left throughout their execution process. While it is imperative that analysts on the development team track, assess, and record feedback and success statuses of QA scripts, engineers should also be directly involved in this review process. Engineer involvement with a QA review is similar to user-driven story development. User-driven story development builds and estimates work from the viewpoint of those using the application. Engineer-driven QA review analyzes the work from the viewpoint of those familiar with the acceptance criteria of the work. As a result, engineer-driven QA increases the likelihood of an application’s success by fulfilling the needs of the use case, business goals, and quantifiable measurements of successful personalization.\n\nSimilarly, an engineer’s involvement in reviewing feedback and UAT results takes this principle further. By reviewing feedback, an engineer will have the opportunity to surface more thorough use cases given by actual users throughout the QA process.\n\nConsider a subscription-based web form that allows users to subscribe themselves and their colleagues to content being delivered by an application. This feature has just been developed, and users have finished their acceptance testing. While nothing indicated a failure or blocker to move code into production, multiple users had left feedback asking for this web form to handle distribution lists and other types of pre-defined mailboxes. Since the engineers were heavily involved with the QA execution of this release, they can quickly refine and estimate new work so that it can be pulled into future sprints. Not only will this save planning time, but it also presents a learning opportunity for the engineers to better understand the process of user-driven development and business cases being solved by the application.\n\nConclusion\n\nUser Acceptance Testing and Quality Assurance plans are foundational to building and delivering an application that fully aligns with the business goals that drive personalization. It allows users to get the most out of the product and ensures the integrity of the application as a whole. This is especially important when considering how personalization affects improvement in content delivery. EK’s advanced content team has experience in a wide range of different areas regarding personalized content delivery. Through our thoughtful approach to QA, we ensure that no use case is left behind and the correct audiences are met with the correct content. Contact us if you see opportunities for personalization in your business!"
    },
    {
        "title": "Knowledge Cast Product Spotlight – Jeff Jonas of Senzing",
        "url": "https://enterprise-knowledge.com/knowledge-cast-product-spotlight-jeff-jonas-of-senzing/",
        "categories": [
            "strategy design"
        ],
        "tags": [
            "ai",
            "knowledge management"
        ],
        "article_type": "podcast",
        "content": "Enterprise Knowledge COO Joe Hilger speaks with Jeff Jonas, Founder and CEO of Senzing. Senzing is a powerful and easy-to-use entity resolution API designed for developers. Senzing software enables organizations of all sizes to gain highly accurate and valuable insights about who is who and who is related to whom in data.\n\nIn this episode, Joe and Jeff discuss the power of conversational search, the rise of large language models, and future of entity resolution.\n\n\n\n﻿ ﻿ ﻿ ﻿\n\n\n\n\n\n\n\n\n\n\n\nIf you would like to be a guest on Knowledge Cast, contact Enterprise Knowledge for more information."
    }
]